<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent</title>
    
    <!-- MathJax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <!-- D3.js -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #f8f9fa;
            color: #2c3e50;
            overflow: hidden;
        }

        .presentation {
            width: 100vw;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .slide {
            display: none;
            width: 90vw;
            max-width: 1100px;
            height: 80vh;
            background: white;
            border-radius: 8px;
            padding: 40px;
            box-shadow: 0 2px 20px rgba(0,0,0,0.1);
            overflow-y: auto;
            position: absolute;
        }

        .slide.active {
            display: block;
            position: relative;
        }

        /* Typography */
        h1 {
            font-size: 2.4rem;
            font-weight: 300;
            color: #2c3e50;
            margin-bottom: 1.5rem;
            line-height: 1.2;
        }

        h2 {
            font-size: 1.9rem;
            font-weight: 300;
            color: #2c3e50;
            margin-bottom: 1.5rem;
            line-height: 1.3;
        }

        h3 {
            font-size: 1.3rem;
            font-weight: 400;
            color: #34495e;
            margin-bottom: 1rem;
        }

        p {
            font-size: 1rem;
            line-height: 1.5;
            color: #5a6c7d;
            margin-bottom: 1.2rem;
        }

        ul {
            list-style: none;
            margin-bottom: 1.5rem;
        }

        li {
            font-size: 1rem;
            line-height: 1.5;
            color: #5a6c7d;
            margin-bottom: 0.6rem;
            padding-left: 1.5rem;
            position: relative;
        }

        li::before {
            content: "•";
            color: #3498db;
            position: absolute;
            left: 0;
            font-size: 1.2rem;
        }

        /* Layout */
        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            align-items: start;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 1.5rem;
            align-items: start;
        }

        /* Content blocks */
        .highlight {
            background: #ecf0f1;
            padding: 1.5rem;
            border-radius: 4px;
            border-left: 4px solid #3498db;
            margin: 1.5rem 0;
        }

        .formula {
            background: #fdfdfd;
            border: 1px solid #e8e8e8;
            border-radius: 4px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            text-align: center;
        }

        .example {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1.2rem;
            margin: 1.2rem 0;
        }

        /* Visualization */
        .viz {
            border: 1px solid #e8e8e8;
            border-radius: 4px;
            padding: 1rem;
            margin: 1.2rem 0;
            background: white;
            min-height: 300px;
        }

        /* Title slide */
        .title-slide {
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }

        .title-slide h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        .subtitle {
            font-size: 1.2rem;
            color: #7f8c8d;
            margin-bottom: 2.5rem;
        }

        .objectives {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 1.5rem;
            max-width: 700px;
            margin: 0 auto;
        }

        .objective {
            background: #f8f9fa;
            padding: 1.2rem;
            border-radius: 4px;
            text-align: left;
        }

        .objective h3 {
            margin-bottom: 0.5rem;
            color: #2c3e50;
        }

        .objective p {
            margin-bottom: 0;
            font-size: 1rem;
        }

        .counter {
            position: fixed;
            bottom: 30px;
            left: 30px;
            color: #7f8c8d;
            font-size: 14px;
        }

        /* Gradient descent specific styles */
        .algorithm-box {
            background: #34495e;
            color: white;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .algorithm-box h4 {
            margin-bottom: 0.5rem;
            font-size: 1.1rem;
        }

        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 1rem;
            margin: 1.5rem 0;
        }

        .comparison-item {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 4px;
            text-align: center;
            border: 2px solid #e9ecef;
        }

        .comparison-item.header {
            background: #3498db;
            color: white;
            font-weight: bold;
        }

        .optimization-path {
            stroke-width: 3px;
            fill: none;
        }

        .gradient-arrow {
            stroke-width: 2px;
            fill: none;
            marker-end: url(#arrowhead);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .slide {
                width: 95vw;
                height: 85vh;
                padding: 25px;
            }
            
            h1 { font-size: 2rem; }
            h2 { font-size: 1.6rem; }
            
            .grid-2, .grid-3, .objectives {
                grid-template-columns: 1fr;
                gap: 1.2rem;
            }
            
            .title-slide h1 {
                font-size: 2.2rem;
            }
        }
    </style>
</head>
<body>
    <div class="presentation">
        <!-- Slide 1: Title -->
        <div class="slide active title-slide" id="slide1">
            <h1>Gradient Descent</h1>
            <p class="subtitle">The Workhorse of Machine Learning Optimization</p>
            
            <div class="objectives">
                <div class="objective">
                    <h3>Mathematical Foundation</h3>
                    <p>Understand gradients and the optimization landscape</p>
                </div>
                <div class="objective">
                    <h3>Batch vs Stochastic</h3>
                    <p>Master different gradient descent variants</p>
                </div>
                <div class="objective">
                    <h3>Mini-batch Strategy</h3>
                    <p>Learn the practical compromise approach</p>
                </div>
                <div class="objective">
                    <h3>Advanced Methods</h3>
                    <p>Explore momentum and adaptive learning rates</p>
                </div>
            </div>
        </div>

        <!-- Slide 2: What is Gradient Descent? -->
        <div class="slide" id="slide2">
            <h2>What is Gradient Descent?</h2>
            
            <div class="highlight">
                <p><strong>Gradient descent is an iterative optimization algorithm that finds the minimum of a function by following the direction of steepest descent, as indicated by the negative gradient.</strong></p>
            </div>
            
            <div class="grid-2">
                <div>
                    <h3>Core Intuition</h3>
                    <ul>
                        <li>Imagine rolling a ball down a hill</li>
                        <li>Ball naturally follows steepest descent</li>
                        <li>Eventually reaches the bottom (minimum)</li>
                        <li>Gradient points uphill, so we go opposite direction</li>
                    </ul>
                    
                    <div class="example">
                        <strong>Real-world Impact:</strong> Gradient descent trains virtually every neural network, from simple perceptrons to GPT and other large language models.
                    </div>
                </div>
                <div>
                    <div class="viz" id="landscape-viz">
                        <div style="height: 100%; display: flex; align-items: center; justify-content: center; color: #95a5a6;">
                            3D Optimization Landscape
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 3: Mathematical Foundation -->
        <div class="slide" id="slide3">
            <h2>Mathematical Foundation</h2>
            
            <div class="grid-2">
                <div>
                    <h3>The Gradient Vector</h3>
                    <div class="formula">
                        $$\nabla f(\boldsymbol{\theta}) = \begin{bmatrix} \frac{\partial f}{\partial \theta_1} \\ \frac{\partial f}{\partial \theta_2} \\ \vdots \\ \frac{\partial f}{\partial \theta_n} \end{bmatrix}$$
                    </div>
                    <p>The gradient points in the direction of steepest increase of the function.</p>
                </div>
                
                <div>
                    <h3>Update Rule</h3>
                    <div class="formula">
                        $$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla f(\boldsymbol{\theta}_t)$$
                    </div>
                    <ul>
                        <li><strong>θ:</strong> Parameters to optimize</li>
                        <li><strong>α:</strong> Learning rate</li>
                        <li><strong>∇f:</strong> Gradient of objective function</li>
                    </ul>
                </div>
            </div>
            
            <div class="highlight">
                <p><strong>Key Insight:</strong> We move in the direction opposite to the gradient because we want to minimize the function, not maximize it.</p>
            </div>
        </div>

        <!-- Slide 4: Learning Rate Effects -->
        <div class="slide" id="slide4">
            <h2>Learning Rate: The Critical Hyperparameter</h2>
            
            <div class="viz" id="learning-rate-viz">
                <div style="height: 300px; display: flex; align-items: center; justify-content: center; color: #95a5a6;">
                    Learning Rate Effects Visualization
                </div>
            </div>
            
            <div class="grid-3">
                <div class="example">
                    <h3>Too Small (α ≪ 1)</h3>
                    <ul>
                        <li>Very slow convergence</li>
                        <li>Many iterations needed</li>
                        <li>Safe but inefficient</li>
                        <li>May get stuck in plateaus</li>
                    </ul>
                </div>
                
                <div class="example">
                    <h3>Just Right (α ≈ optimal)</h3>
                    <ul>
                        <li>Smooth, fast convergence</li>
                        <li>Efficient optimization</li>
                        <li>Reaches minimum quickly</li>
                        <li>Problem-dependent value</li>
                    </ul>
                </div>
                
                <div class="example">
                    <h3>Too Large (α ≫ 1)</h3>
                    <ul>
                        <li>Oscillating behavior</li>
                        <li>May overshoot minimum</li>
                        <li>Potential divergence</li>
                        <li>Unstable optimization</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 5: Batch Gradient Descent -->
        <div class="slide" id="slide5">
            <h2>Batch Gradient Descent</h2>
            
            <div class="highlight">
                <h4>Algorithm: Batch Gradient Descent</h4>
                <p>Compute gradient using entire training dataset at each step</p>
            </div>
            
            <div class="formula">
                $$\nabla f(\boldsymbol{\theta}) = \frac{1}{n}\sum_{i=1}^{n} \nabla f_i(\boldsymbol{\theta})$$
            </div>
            
            <div class="grid-2">
                <div>
                    <h3>Advantages</h3>
                    <ul>
                        <li>Guaranteed convergence for convex functions</li>
                        <li>Smooth convergence path</li>
                        <li>Stable gradient estimates</li>
                        <li>Theoretical guarantees</li>
                    </ul>
                </div>
                
                <div>
                    <h3>Disadvantages</h3>
                    <ul>
                        <li>Computationally expensive per iteration</li>
                        <li>Memory intensive for large datasets</li>
                        <li>Slow for big data applications</li>
                        <li>May get stuck in local minima</li>
                    </ul>
                </div>
            </div>
            
            <div class="example">
                <strong>When to Use:</strong> Small to medium datasets, when you need precise convergence, or when computational resources are abundant.
            </div>
        </div>

        <!-- Slide 6: Stochastic Gradient Descent -->
        <div class="slide" id="slide6">
            <h2>Stochastic Gradient Descent (SGD)</h2>
            
            <div class="highlight">
                <h4>Algorithm: Stochastic Gradient Descent</h4>
                <p>Use gradient from single randomly selected sample at each step</p>
            </div>
            
            <div class="formula">
                $$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla f_i(\boldsymbol{\theta}_t)$$
            </div>
            
            <div class="grid-2">
                <div>
                    <h3>Advantages</h3>
                    <ul>
                        <li>Fast iterations (one sample per update)</li>
                        <li>Low memory requirements</li>
                        <li>Can escape local minima due to noise</li>
                        <li>Online learning capability</li>
                        <li>Scales to massive datasets</li>
                    </ul>
                </div>
                
                <div>
                    <h3>Disadvantages</h3>
                    <ul>
                        <li>Noisy convergence path</li>
                        <li>May not converge to exact minimum</li>
                        <li>Requires learning rate decay</li>
                        <li>Sensitive to hyperparameters</li>
                    </ul>
                </div>
            </div>
            
            <div class="viz" id="sgd-path-viz">
                <div style="height: 200px; display: flex; align-items: center; justify-content: center; color: #95a5a6;">
                    SGD Convergence Path (Noisy)
                </div>
            </div>
        </div>

        <!-- Slide 7: Mini-batch Gradient Descent -->
        <div class="slide" id="slide7">
            <h2>Mini-batch Gradient Descent</h2>
            
            <div class="highlight">
                <p><strong>Mini-batch gradient descent combines the best of both worlds: computational efficiency of SGD with stability of batch gradient descent.</strong></p>
            </div>
            
            <div class="formula">
                $$\nabla f(\boldsymbol{\theta}) = \frac{1}{m}\sum_{i \in \mathcal{B}} \nabla f_i(\boldsymbol{\theta})$$
            </div>
            
            <div class="grid-2">
                <div>
                    <h3>Batch Size Selection</h3>
                    <ul>
                        <li><strong>Small batches (32-128):</strong> More noise, faster iterations</li>
                        <li><strong>Medium batches (256-512):</strong> Good balance for most problems</li>
                        <li><strong>Large batches (1024+):</strong> More stable, requires more memory</li>
                    </ul>
                    
                    <div class="example">
                        <strong>Rule of Thumb:</strong> Start with batch size 32-256, increase if you have computational resources and want stability.
                    </div>
                </div>
                
                <div>
                    <div class="viz" id="minibatch-comparison">
                        <div style="height: 250px; display: flex; align-items: center; justify-content: center; color: #95a5a6;">
                            Batch Size Effects Comparison
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 8: Comparison of Methods -->
        <div class="slide" id="slide8">
            <h2>Gradient Descent Variants Comparison</h2>
            
            <div class="comparison-grid">
                <div class="comparison-item header">Aspect</div>
                <div class="comparison-item header">Batch GD</div>
                <div class="comparison-item header">SGD</div>
                
                <div class="comparison-item"><strong>Computation per Iteration</strong></div>
                <div class="comparison-item">O(n) - entire dataset</div>
                <div class="comparison-item">O(1) - single sample</div>
                
                <div class="comparison-item"><strong>Memory Usage</strong></div>
                <div class="comparison-item">High - stores all gradients</div>
                <div class="comparison-item">Low - one sample at a time</div>
                
                <div class="comparison-item"><strong>Convergence</strong></div>
                <div class="comparison-item">Smooth and stable</div>
                <div class="comparison-item">Noisy but can escape local minima</div>
                
                <div class="comparison-item"><strong>Parallelization</strong></div>
                <div class="comparison-item">✓ Easy to parallelize</div>
                <div class="comparison-item">✗ Sequential by nature</div>
                
                <div class="comparison-item"><strong>Large Datasets</strong></div>
                <div class="comparison-item">✗ Impractical</div>
                <div class="comparison-item">✓ Excellent scalability</div>
                
                <div class="comparison-item"><strong>Online Learning</strong></div>
                <div class="comparison-item">✗ Not suitable</div>
                <div class="comparison-item">✓ Perfect fit</div>
            </div>
            
            <div class="example">
                <strong>Mini-batch GD:</strong> Offers a practical compromise with batch sizes typically between 32-512, combining efficiency with stability.
            </div>
        </div>

        <!-- Slide 9: Advanced Variants -->
        <div class="slide" id="slide9">
            <h2>Advanced Gradient Descent Methods</h2>
            
            <div class="grid-2">
                <div>
                    <h3>SGD with Momentum</h3>
                    <div class="formula">
                        $$\mathbf{v}_t = \beta \mathbf{v}_{t-1} + \alpha \nabla f(\boldsymbol{\theta}_t)$$
                        $$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \mathbf{v}_t$$
                    </div>
                    <p>Accumulates velocity to accelerate in consistent directions and dampen oscillations.</p>
                    
                    <h3>Adam Optimizer</h3>
                    <div class="formula">
                        $$\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + (1-\beta_1)\nabla f(\boldsymbol{\theta}_t)$$
                        $$\mathbf{v}_t = \beta_2 \mathbf{v}_{t-1} + (1-\beta_2)(\nabla f(\boldsymbol{\theta}_t))^2$$
                    </div>
                    <p>Combines momentum with adaptive learning rates for each parameter.</p>
                </div>
                
                <div>
                    <h3>When to Use Each Method</h3>
                    <ul>
                        <li><strong>Vanilla SGD:</strong> Simple problems, when you want full control</li>
                        <li><strong>SGD + Momentum:</strong> Training deep networks, accelerated convergence</li>
                        <li><strong>Adam:</strong> Default choice for most deep learning applications</li>
                        <li><strong>RMSprop:</strong> RNNs and problems with sparse gradients</li>
                    </ul>
                    
                    <div class="example">
                        <h4>Practical Advice</h4>
                        <p>Start with Adam (α=0.001) for most problems. Use SGD+momentum for fine-tuning or when you need more control.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 10: Summary -->
        <div class="slide" id="slide10">
            <h2>Key Takeaways</h2>
            
            <div class="grid-2">
                <div>
                    <h3>Fundamental Principles</h3>
                    <ul>
                        <li>Gradient descent follows the steepest descent direction</li>
                        <li>Learning rate controls step size and convergence</li>
                        <li>Batch size affects stability vs. computational efficiency</li>
                        <li>Different variants suit different problem scales</li>
                    </ul>
                </div>
                
                <div>
                    <h3>Practical Guidelines</h3>
                    <ul>
                        <li>Use mini-batch GD for most practical applications</li>
                        <li>Start with Adam optimizer for deep learning</li>
                        <li>Tune learning rate carefully - often most important hyperparameter</li>
                        <li>Consider momentum for faster convergence</li>
                    </ul>
                </div>
            </div>
            
            <div class="highlight">
                <p><strong>Gradient descent is the foundation of machine learning optimization. Understanding its variants helps you choose the right tool for your specific problem constraints and computational resources.</strong></p>
            </div>
            
            <div class="example">
                <strong>Modern Reality:</strong> While frameworks automate implementation, understanding these principles helps you debug convergence issues, select appropriate optimizers, and tune hyperparameters effectively.
            </div>
        </div>
    </div>

    <!-- Navigation -->
    <div class="counter">
        <span id="current">1</span> / <span id="total">10</span>
    </div>

    <script>
        let currentSlide = 1;
        const totalSlides = 10;

        function showSlide(n) {
            const slides = document.querySelectorAll('.slide');
            if (n > totalSlides) currentSlide = 1;
            if (n < 1) currentSlide = totalSlides;
            
            slides.forEach(slide => slide.classList.remove('active'));
            slides[currentSlide - 1].classList.add('active');
            
            document.getElementById('current').textContent = currentSlide;
            
            // Initialize visualizations
            setTimeout(() => initViz(currentSlide), 100);
        }

        function nextSlide() {
            if (currentSlide < totalSlides) {
                currentSlide++;
                showSlide(currentSlide);
            }
        }

        function previousSlide() {
            if (currentSlide > 1) {
                currentSlide--;
                showSlide(currentSlide);
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowRight' || e.key === ' ') nextSlide();
            if (e.key === 'ArrowLeft') previousSlide();
        });

        // Visualization functions
        function initViz(slideNum) {
            console.log('Initializing visualization for slide:', slideNum);
            
            try {
                switch(slideNum) {
                    case 2:
                        createLandscapeViz();
                        break;
                    case 4:
                        createLearningRateViz();
                        break;
                    case 6:
                        createSGDPathViz();
                        break;
                    case 7:
                        createMinibatchComparison();
                        break;
                }
            } catch (error) {
                console.error('Visualization error:', error);
            }
        }

        function createLandscapeViz() {
            const container = d3.select('#landscape-viz');
            container.selectAll('*').remove();
            
            const width = 400;
            const height = 250;
            
            const svg = container
                .append('svg')
                .attr('width', width)
                .attr('height', height);
            
            // Create contour plot for optimization landscape
            const xData = d3.range(-3, 3.1, 0.2);
            const yData = d3.range(-3, 3.1, 0.2);
            
            const xScale = d3.scaleLinear().domain([-3, 3]).range([50, width - 50]);
            const yScale = d3.scaleLinear().domain([-3, 3]).range([height - 50, 50]);
            
            // Create contour lines
            const contourLevels = [0.5, 1, 2, 4, 8];
            contourLevels.forEach((level, i) => {
                svg.append('ellipse')
                    .attr('cx', xScale(0.5))
                    .attr('cy', yScale(-0.5))
                    .attr('rx', 30 + level * 15)
                    .attr('ry', 20 + level * 10)
                    .attr('fill', 'none')
                    .attr('stroke', '#3498db')
                    .attr('stroke-width', 2)
                    .attr('opacity', 1 - i * 0.15);
            });
            
            // Add optimization path
            const pathData = [
                [-2, 2], [-1.5, 1.2], [-1, 0.5], [-0.5, 0], [0.2, -0.3], [0.5, -0.5]
            ];
            
            const line = d3.line()
                .x(d => xScale(d[0]))
                .y(d => yScale(d[1]))
                .curve(d3.curveCardinal);
            
            svg.append('path')
                .datum(pathData)
                .attr('d', line)
                .attr('stroke', '#e74c3c')
                .attr('stroke-width', 3)
                .attr('fill', 'none')
                .attr('marker-end', 'url(#arrowhead)');
            
            // Add arrow marker
            svg.append('defs')
                .append('marker')
                .attr('id', 'arrowhead')
                .attr('markerWidth', 10)
                .attr('markerHeight', 7)
                .attr('refX', 9)
                .attr('refY', 3.5)
                .attr('orient', 'auto')
                .append('polygon')
                .attr('points', '0 0, 10 3.5, 0 7')
                .attr('fill', '#e74c3c');
            
            // Mark minimum
            svg.append('circle')
                .attr('cx', xScale(0.5))
                .attr('cy', yScale(-0.5))
                .attr('r', 6)
                .attr('fill', '#f39c12')
                .attr('stroke', '#d35400')
                .attr('stroke-width', 2);
            
            // Add labels
            svg.append('text')
                .attr('x', width/2)
                .attr('y', height - 10)
                .text('Optimization Landscape')
                .attr('text-anchor', 'middle')
                .attr('font-size', '14px')
                .attr('font-weight', 'bold');
        }

        function createLearningRateViz() {
            const container = d3.select('#learning-rate-viz');
            container.selectAll('*').remove();
            
            const width = 500;
            const height = 250;
            const margin = {top: 20, right: 100, bottom: 40, left: 50};
            
            const svg = container
                .append('svg')
                .attr('width', width)
                .attr('height', height);
            
            // Generate convergence data for different learning rates
            const iterations = d3.range(0, 50);
            const targetValue = 0;
            
            const smallLR = iterations.map(i => 10 * Math.exp(-0.05 * i) + 0.2 * Math.random());
            const goodLR = iterations.map(i => 10 * Math.exp(-0.15 * i) + 0.1 * Math.random());
            const largeLR = iterations.map(i => 10 * Math.exp(-0.1 * i) * Math.cos(0.3 * i) + 0.3 * Math.random());
            
            const xScale = d3.scaleLinear()
                .domain([0, 49])
                .range([margin.left, width - margin.right]);
            
            const yScale = d3.scaleLinear()
                .domain([0, 12])
                .range([height - margin.bottom, margin.top]);
            
            const line = d3.line()
                .x((d, i) => xScale(i))
                .y(d => yScale(d));
            
            // Draw convergence paths
            svg.append('path')
                .datum(smallLR)
                .attr('d', line)
                .attr('stroke', '#3498db')
                .attr('stroke-width', 3)
                .attr('fill', 'none');
            
            svg.append('path')
                .datum(goodLR)
                .attr('d', line)
                .attr('stroke', '#2ecc71')
                .attr('stroke-width', 3)
                .attr('fill', 'none');
            
            svg.append('path')
                .datum(largeLR)
                .attr('d', line)
                .attr('stroke', '#e74c3c')
                .attr('stroke-width', 3)
                .attr('fill', 'none');
            
            // Add legend
            const legend = svg.append('g')
                .attr('transform', `translate(${width - 90}, 40)`);
            
            legend.append('line').attr('x1', 0).attr('x2', 20).attr('y1', 0).attr('y2', 0)
                .attr('stroke', '#3498db').attr('stroke-width', 3);
            legend.append('text').attr('x', 25).attr('y', 5).text('Too Small').attr('font-size', '12px');
            
            legend.append('line').attr('x1', 0).attr('x2', 20).attr('y1', 20).attr('y2', 20)
                .attr('stroke', '#2ecc71').attr('stroke-width', 3);
            legend.append('text').attr('x', 25).attr('y', 25).text('Just Right').attr('font-size', '12px');
            
            legend.append('line').attr('x1', 0).attr('x2', 20).attr('y1', 40).attr('y2', 40)
                .attr('stroke', '#e74c3c').attr('stroke-width', 3);
            legend.append('text').attr('x', 25).attr('y', 45).text('Too Large').attr('font-size', '12px');
            
            // Add axes labels
            svg.append('text')
                .attr('x', width/2)
                .attr('y', height - 5)
                .text('Iterations')
                .attr('text-anchor', 'middle')
                .attr('font-size', '12px');
        }

        function createSGDPathViz() {
            const container = d3.select('#sgd-path-viz');
            container.selectAll('*').remove();
            
            const width = 400;
            const height = 150;
            
            const svg = container
                .append('svg')
                .attr('width', width)
                .attr('height', height);
            
            // Generate noisy SGD path
            const steps = 50;
            const pathData = [];
            let x = -2, y = 1.5;
            
            for (let i = 0; i < steps; i++) {
                pathData.push([x, y]);
                // Move towards optimum with noise
                x += 0.08 + 0.1 * (Math.random() - 0.5);
                y -= 0.06 + 0.08 * (Math.random() - 0.5);
            }
            
            const xScale = d3.scaleLinear().domain([-2, 3]).range([20, width - 20]);
            const yScale = d3.scaleLinear().domain([-1, 2]).range([height - 20, 20]);
            
            const line = d3.line()
                .x(d => xScale(d[0]))
                .y(d => yScale(d[1]));
            
            svg.append('path')
                .datum(pathData)
                .attr('d', line)
                .attr('stroke', '#e74c3c')
                .attr('stroke-width', 2)
                .attr('fill', 'none');
            
            // Add start and end points
            svg.append('circle')
                .attr('cx', xScale(pathData[0][0]))
                .attr('cy', yScale(pathData[0][1]))
                .attr('r', 5)
                .attr('fill', '#3498db');
            
            svg.append('circle')
                .attr('cx', xScale(pathData[pathData.length-1][0]))
                .attr('cy', yScale(pathData[pathData.length-1][1]))
                .attr('r', 5)
                .attr('fill', '#f39c12');
            
            // Labels
            svg.append('text')
                .attr('x', 25)
                .attr('y', 15)
                .text('Start')
                .attr('fill', '#3498db')
                .attr('font-size', '12px');
            
            svg.append('text')
                .attr('x', width - 50)
                .attr('y', height - 10)
                .text('End')
                .attr('fill', '#f39c12')
                .attr('font-size', '12px');
        }

        function createMinibatchComparison() {
            const container = d3.select('#minibatch-comparison');
            container.selectAll('*').remove();
            
            const width = 400;
            const height = 200;
            const margin = {top: 20, right: 20, bottom: 40, left: 50};
            
            const svg = container
                .append('svg')
                .attr('width', width)
                .attr('height', height);
            
            // Generate data for different batch sizes
            const iterations = d3.range(0, 30);
            const batch32 = iterations.map(i => 5 * Math.exp(-0.12 * i) + 0.3 * Math.random());
            const batch128 = iterations.map(i => 5 * Math.exp(-0.1 * i) + 0.15 * Math.random());
            const batch512 = iterations.map(i => 5 * Math.exp(-0.08 * i) + 0.08 * Math.random());
            
            const xScale = d3.scaleLinear()
                .domain([0, 29])
                .range([margin.left, width - margin.right]);
            
            const yScale = d3.scaleLinear()
                .domain([0, 6])
                .range([height - margin.bottom, margin.top]);
            
            const line = d3.line()
                .x((d, i) => xScale(i))
                .y(d => yScale(d));
            
            // Draw convergence curves
            svg.append('path')
                .datum(batch32)
                .attr('d', line)
                .attr('stroke', '#e74c3c')
                .attr('stroke-width', 2)
                .attr('fill', 'none');
            
            svg.append('path')
                .datum(batch128)
                .attr('d', line)
                .attr('stroke', '#f39c12')
                .attr('stroke-width', 2)
                .attr('fill', 'none');
            
            svg.append('path')
                .datum(batch512)
                .attr('d', line)
                .attr('stroke', '#2ecc71')
                .attr('stroke-width', 2)
                .attr('fill', 'none');
            
            // Add legend
            const legend = svg.append('g')
                .attr('transform', `translate(${width - 100}, 30)`);
            
            ['Batch 32', 'Batch 128', 'Batch 512'].forEach((text, i) => {
                const colors = ['#e74c3c', '#f39c12', '#2ecc71'];
                legend.append('line')
                    .attr('x1', 0).attr('x2', 15)
                    .attr('y1', i * 15).attr('y2', i * 15)
                    .attr('stroke', colors[i]).attr('stroke-width', 2);
                legend.append('text')
                    .attr('x', 20).attr('y', i * 15 + 4)
                    .text(text).attr('font-size', '10px');
            });
        }

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            console.log('DOM loaded, initializing...');
            showSlide(1);
        });
    </script>
</body>
</html>
