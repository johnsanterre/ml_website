{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4: Vector Representations & Similarity - Homework\n",
        "\n",
        "**ML2: Advanced Machine Learning**\n",
        "\n",
        "**Estimated Time**: 1 hour\n",
        "\n",
        "---\n",
        "\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Programming Exercises (60%)\n",
        "\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Experiment: Cosine vs Euclidean Similarity\n",
        "\n",
        "**Time**: 10 min\n",
        "\n",
        "Observe how cosine similarity and Euclidean distance behave differently, especially with vector magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# User preferences (ratings 1-5 for 5 movies)\n",
        "user_a = np.array([5, 5, 1, 1, 1])  # Loves action, hates romance\n",
        "user_b = np.array([4, 4, 1, 1, 1])  # Same pattern, slightly lower ratings\n",
        "user_c = np.array([1, 1, 5, 5, 5])  # Opposite preferences\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def euclidean_distance(v1, v2):\n",
        "    return np.linalg.norm(v1 - v2)\n",
        "\n",
        "print(\"User A vs User B:\")\n",
        "print(f\"  Cosine similarity: {cosine_similarity(user_a, user_b):.4f}\")\n",
        "print(f\"  Euclidean distance: {euclidean_distance(user_a, user_b):.4f}\")\n",
        "\n",
        "print(\"\\nUser A vs User C:\")\n",
        "print(f\"  Cosine similarity: {cosine_similarity(user_a, user_c):.4f}\")\n",
        "print(f\"  Euclidean distance: {euclidean_distance(user_a, user_c):.4f}\")\n",
        "\n",
        "# Now scale user_b by 2x (enthusiastic rater)\n",
        "user_b_scaled = user_b * 2\n",
        "\n",
        "print(\"\\nUser A vs User B (2x scaled):\")\n",
        "print(f\"  Cosine similarity: {cosine_similarity(user_a, user_b_scaled):.4f}\")\n",
        "print(f\"  Euclidean distance: {euclidean_distance(user_a, user_b_scaled):.4f}\")\n",
        "\n",
        "# TODO: Answer reflection questions about what this tells you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Experiment: Sparsity Problem\n",
        "\n",
        "**Time**: 8 min\n",
        "\n",
        "See how sparse vectors (mostly zeros) make similarity search difficult."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Sparse binary vectors (1000 dimensions, only 10 are 1)\n",
        "def create_sparse_vector(dim=1000, num_ones=10):\n",
        "    vec = np.zeros(dim)\n",
        "    indices = np.random.choice(dim, num_ones, replace=False)\n",
        "    vec[indices] = 1\n",
        "    return vec\n",
        "\n",
        "# Create 5 users with sparse preferences\n",
        "users = [create_sparse_vector() for _ in range(5)]\n",
        "\n",
        "# Compute pairwise similarities\n",
        "print(\"Pairwise Cosine Similarities:\")\n",
        "for i in range(len(users)):\n",
        "    for j in range(i+1, len(users)):\n",
        "        sim = np.dot(users[i], users[j]) / (np.linalg.norm(users[i]) * np.linalg.norm(users[j]))\n",
        "        overlap = int(np.dot(users[i], users[j]))  # Number of shared 1s\n",
        "        print(f\"User {i} vs User {j}: similarity={sim:.4f}, shared items={overlap}\")\n",
        "\n",
        "# TODO: What pattern do you notice? Why is this a problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Knowledge Questions (40%)\n",
        "\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Short Answer)\n",
        "\n",
        "**Question 1 - Cosine vs Euclidean Conceptual Understanding**\n\nCosine similarity measures the ANGLE between vectors.\nEuclidean distance measures the MAGNITUDE of difference.\n\nExplain:\n1. Why is cosine similarity better for comparing user preferences?\n2. Give an example where two users have the same taste but different cosine vs Euclidean similarity.\n3. What does a cosine similarity of 1.0 mean? What about -1.0?\n",
        "\n",
        "**Hint**: Think about rating scales. One user might rate everything 1-5, another 2-4, but have the same preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\n",
        "\n",
        "**Question 2 - Experiment Reflection: Scaling**\n\nIn the 'Cosine vs Euclidean' experiment, User B was scaled by 2x (all ratings doubled).\n\nObserve what happened to:\n1. Cosine similarity (did it change?)\n2. Euclidean distance (did it change?)\n3. Which metric correctly recognizes that User A and User B (scaled) have the SAME preferences?\n",
        "\n",
        "**Hint**: Cosine is scale-invariant. It only cares about direction (pattern), not magnitude (enthusiasm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3 (Multiple Choice)\n",
        "\n",
        "**Question 3 - When to Use Which Metric**\n\nYou're building a movie recommendation system. Users rate movies 1-5 stars. Some users are \"easy graders\" (average 4.0) and some are \"harsh critics\" (average 2.5), but they might have similar taste.\n\nWhich similarity metric should you use?\n\nA) Euclidean distance\nB) Cosine similarity\nC) Manhattan distance\nD) Doesn't matter\n",
        "\n",
        "A) Euclidean distance\n",
        "B) Cosine similarity\n",
        "C) Manhattan distance\n",
        "D) Doesn't matter\n",
        "\n",
        "**Hint**: You want to capture PREFERENCE PATTERNS, not absolute rating levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4 (Short Answer)\n",
        "\n",
        "**Question 4 - The Sparsity Problem**\n\nBased on the 'Sparsity Problem' experiment:\n\nWith 1000 possible items and users only rating 10 items each, most users had NO shared items (overlap=0).\n\nExplain:\n1. Why does sparsity make similarity computation difficult?\n2. What happens to cosine similarity when two vectors share no non-zero elements?\n3. How might you solve this problem in practice?\n",
        "\n",
        "**Hint**: If vectors don't overlap, you can't find similarity even if users actually have similar tastes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5 (Short Answer)\n",
        "\n",
        "**Question 5 - Dimensionality Reduction Motivation**\n\nHigh-dimensional sparse vectors → hard to find similarities\nLow-dimensional dense vectors → easier to find patterns\n\nExplain: How could you transform high-dimensional sparse movie ratings into low-dimensional dense embeddings? What would the embeddings capture?\n",
        "\n",
        "**Hint**: Think about learning latent factors like 'action fan' or 'romance lover' instead of storing raw ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6 (Multiple Choice)\n",
        "\n",
        "**Question 6 - Curse of Dimensionality**\n\nIn very high dimensions (e.g., 10,000), strange things happen to distances. Most points appear roughly equidistant from each other.\n\nWhy is this a problem for nearest neighbor search?\n\nA) It makes computation slow\nB) It makes all similarities look the same (less informative)\nC) It uses too much memory\nD) It requires more training data\n",
        "\n",
        "A) It makes computation slow\n",
        "B) It makes all similarities look the same (less informative)\n",
        "C) It uses too much memory\n",
        "D) It requires more training data\n",
        "\n",
        "**Hint**: When everything is equally far apart, you can't distinguish near from far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7 (Short Answer)\n",
        "\n",
        "**Question 7 - Learned Representations vs Manual Features**\n\nManual approach: Design features like \"likes action\", \"likes romance\" (requires domain expertise)\nLearned approach: Let a neural network discover features automatically from data\n\nExplain:\n1. What advantage does the learned approach have?\n2. What disadvantage might it have?\n3. When would you prefer manual features?\n",
        "\n",
        "**Hint**: Learned = automatic but less interpretable. Manual = interpretable but requires expertise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8 (Short Answer)\n",
        "\n",
        "**Question 8 - Dot Product Interpretation**\n\nCosine similarity = (A · B) / (||A|| × ||B||)\n\nThe numerator is the dot product A · B.\n\nExplain in intuitive terms: What does a high dot product between two vectors mean? What about a dot product of zero?\n",
        "\n",
        "**Hint**: Dot product measures alignment. High = pointing same direction. Zero = perpendicular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9 (Short Answer)\n",
        "\n",
        "**Question 9 - Normalization Impact**\n\nIf you normalize all vectors to unit length (magnitude 1), what happens to the relationship between cosine similarity and Euclidean distance?\n\nHint: Try the math with ||A|| = ||B|| = 1\n",
        "\n",
        "**Hint**: When vectors are normalized, cosine similarity and Euclidean distance become related."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 10 (Short Answer)\n",
        "\n",
        "**Question 10 - Real-World Application**\n\nSpotify has 100 million songs and wants to find \"similar songs\" for recommendations.\n\nExplain:\n1. Why can't they store a 100M × 100M similarity matrix?\n2. How do learned embeddings (e.g., 128-dimensional vectors per song) solve this?\n3. What tradeoff are they making?\n",
        "\n",
        "**Hint**: 100M × 100M floats = massive storage. 100M × 128 floats = much smaller. But you lose some information in compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission\n",
        "\n",
        "Before submitting:\n",
        "1. Run all cells to ensure code executes without errors\n",
        "2. Check that all questions are answered\n",
        "3. Review your explanations for clarity\n",
        "\n",
        "**To Submit**:\n",
        "- File → Download → Download .ipynb\n",
        "- Submit the notebook file to your course LMS\n",
        "\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}