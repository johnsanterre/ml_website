{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 13: Evaluating LLMs - Metrics and Methods - Homework\n",
        "\n",
        "**ML2: Advanced Machine Learning**\n",
        "\n",
        "**Estimated Time**: 1 hour\n",
        "\n",
        "---\n",
        "\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Programming Exercises (60%)\n",
        "\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Experiment: Why Metrics Disagree\n",
        "\n",
        "**Time**: 10 min\n",
        "\n",
        "Compare different metrics on the same outputs and observe disagreements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Three model outputs for \"Summarize: The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "reference = \"A fast fox jumps over a lazy dog.\"\n",
        "\n",
        "candidate_a = \"A quick brown fox jumps over a lazy dog.\"  # Almost exact\n",
        "candidate_b = \"A fox leaps over a dog.\"  # Concise, captures meaning\n",
        "candidate_c = \"The agile fox bounds over the sleepy canine.\"  # Different words, same meaning\n",
        "\n",
        "# Simplified BLEU (measures n-gram overlap)\n",
        "def simple_bleu(ref, cand):\n",
        "    ref_words = set(ref.lower().split())\n",
        "    cand_words = set(cand.lower().split())\n",
        "    overlap = len(ref_words & cand_words) / len(cand_words) if cand_words else 0\n",
        "    return overlap\n",
        "\n",
        "print(f\"Candidate A BLEU: {simple_bleu(reference, candidate_a):.2f}\")\n",
        "print(f\"Candidate B BLEU: {simple_bleu(reference, candidate_b):.2f}\")\n",
        "print(f\"Candidate C BLEU: {simple_bleu(reference, candidate_c):.2f}\")\n",
        "\n",
        "# TODO: Which is best? Do the metrics agree with human judgment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Knowledge Questions (40%)\n",
        "\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Short Answer)\n",
        "\n",
        "**Question 1 - No Single Metric Captures Everything**\n\nBLEU: Measures word overlap\nROUGE: Measures recall of n-grams\nBERTScore: Measures semantic similarity using embeddings\nHuman evaluation: Subjective quality judgment\n\nExplain:\n1. Why might BLEU give a high score to a bad summary?\n2. Why might a paraphrase (different words, same meaning) score poorly on BLEU?\n3. Why do we need multiple metrics?\n",
        "\n",
        "**Hint**: BLEU rewards literal word matches, ignoring semantics. Multiple metrics = multiple perspectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\n",
        "\n",
        "**Question 2 - Perplexity**\n\nPerplexity = how \"surprised\" a language model is by text.\nLow perplexity = model assigned high probability to the text.\n\nExplain:\n1. Why is perplexity a good metric for language modeling?\n2. Can you use perplexity to evaluate summarization quality? Why or why not?\n3. What are the limitations?\n",
        "\n",
        "**Hint**: Perplexity measures likelihood, not quality/helpfulness/correctness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3 (Multiple Choice)\n",
        "\n",
        "**Question 3 - BLEU Score**\n\nBLEU was invented for machine translation. It measures n-gram overlap between reference and candidate.\n\nBLEU = 1.0 means:\n\nA) Perfect translation\nB) Exact word-for-word match with reference\nC) The model is 100% confident\nD) Zero perplexity\n",
        "\n",
        "A) Perfect translation\n",
        "B) Exact word-for-word match with reference\n",
        "C) The model is 100% confident\n",
        "D) Zero perplexity\n",
        "\n",
        "**Hint**: BLEU = 1.0 means perfect n-gram overlap, not necessarily perfect translation quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4 (Short Answer)\n",
        "\n",
        "**Question 4 - Human Evaluation**\n\nHuman evaluation is considered the \"gold standard\" but it's expensive and slow.\n\nExplain:\n1. What aspects can humans evaluate that automatic metrics cannot?\n2. What problems arise with human evaluation (inter-annotator agreement, bias)?\n3. When is human evaluation worth the cost?\n",
        "\n",
        "**Hint**: Humans assess fluency, coherence, factuality, helpfulness. But humans disagree and have biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5 (Short Answer)\n",
        "\n",
        "**Question 5 - Benchmark Saturation**\n\nGPT-4 achieves near-perfect scores on many NLP benchmarks (GLUE, SuperGLUE).\n\nExplain:\n1. Why is benchmark saturation a problem?\n2. What does it mean when models \"solve\" a benchmark?\n3. How do researchers respond? (Hint: new, harder benchmarks)\n",
        "\n",
        "**Hint**: Saturated benchmarks no longer differentiate model quality. Need harder tests that reflect real-world use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6 (Multiple Choice)\n",
        "\n",
        "**Question 6 - BERTScore vs BLEU**\n\nBERTScore uses BERT embeddings to measure semantic similarity.\nBLEU uses n-gram overlap.\n\nWhich would score higher for: \"The cat sat\" vs \"The feline rested\"?\n\nA) BLEU (they have very different words)\nB) BERTScore (semantically similar)\nC) Both equally\nD) Neither would score it\n",
        "\n",
        "A) BLEU (they have very different words)\n",
        "B) BERTScore (semantically similar)\n",
        "C) Both equally\n",
        "D) Neither would score it\n",
        "\n",
        "**Hint**: BERTScore captures semantic similarity. \"cat\" ≈ \"feline\" in embedding space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7 (Short Answer)\n",
        "\n",
        "**Question 7 - LLM-as-Judge**\n\nNew trend: Use GPT-4 to evaluate other models' outputs.\n\nPrompt: \"Rate this summary on a scale of 1-10 for accuracy and coherence.\"\n\nExplain:\n1. What advantages does this have over BLEU/ROUGE?\n2. What risks/limitations exist?\n3. When is this approach appropriate?\n",
        "\n",
        "**Hint**: Advantages: nuanced, flexible criteria. Risks: GPT-4 has biases, can be manipulated, costly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8 (Short Answer)\n",
        "\n",
        "**Question 8 - Task-Specific Evaluation**\n\nFor a medical diagnosis LLM:\n- Accuracy on medical exams\n- Factual correctness\n- Safety (avoiding harmful advice)\n\nExplain: Why are general metrics (BLEU, perplexity) insufficient for this use case?\n",
        "\n",
        "**Hint**: Domain-specific tasks need domain-specific evaluation criteria (medical accuracy, safety)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9 (Short Answer)\n",
        "\n",
        "**Question 9 - Evaluation Dimensions**\n\nLLMs should be evaluated on multiple dimensions:\n- Helpfulness: Does it solve the user's problem?\n- Truthfulness: Is it factually accurate?\n- Harmlessness: Does it avoid harmful content?\n\nExplain: Why might a model excel on one dimension but fail on another?\n",
        "\n",
        "**Hint**: Trade-offs exist. A very helpful model might hallucinate. A very cautious model might refuse valid requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 10 (Short Answer)\n",
        "\n",
        "**Question 10 - Evaluation Paradox**\n\nAs models get better, evaluation gets harder.\n\nGPT-4's outputs are often indistinguishable from humans. How do you evaluate something when there's no clear \"right answer\"?\n\nExplain: What new evaluation approaches are needed for super-human AI?\n",
        "\n",
        "**Hint**: Need process-based evaluation (how it thinks), adversarial testing, long-term outcome measurement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission\n",
        "\n",
        "Before submitting:\n",
        "1. Run all cells to ensure code executes without errors\n",
        "2. Check that all questions are answered\n",
        "3. Review your explanations for clarity\n",
        "\n",
        "**To Submit**:\n",
        "- File → Download → Download .ipynb\n",
        "- Submit the notebook file to your course LMS\n",
        "\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}