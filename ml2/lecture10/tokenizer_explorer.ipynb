{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Explorer\n",
    "\n",
    "Before an LLM reads a single word, it breaks text into **tokens**. This notebook makes that process visible.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Why tokenization exists\n",
    "2. How to build a simple tokenizer from scratch\n",
    "3. How GPT-style BPE tokenization works\n",
    "4. Surprising edge cases that affect LLM behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Not Just Use Characters or Words?\n",
    "\n",
    "The naive approaches both have serious problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The unbelievably fast tokenizer splits text efficiently.\"\n",
    "\n",
    "# Approach 1: Character-level\n",
    "char_tokens = list(sentence)\n",
    "print(\"Character-level tokenization:\")\n",
    "print(char_tokens)\n",
    "print(f\"Token count: {len(char_tokens)}\")\n",
    "print(\"Problem: sequences become very long, hard to learn meaning from single chars\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Approach 2: Word-level\n",
    "word_tokens = sentence.split()\n",
    "print(\"Word-level tokenization:\")\n",
    "print(word_tokens)\n",
    "print(f\"Token count: {len(word_tokens)}\")\n",
    "print(\"Problem: 'unbelievable', 'unbelievably', 'unbelievably' are all different tokens\")\n",
    "print(\"         vocabulary explodes, rare words become unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build BPE From Scratch\n",
    "\n",
    "**Byte Pair Encoding (BPE)** solves this by starting with characters and merging the most frequent pairs repeatedly.\n",
    "\n",
    "It finds a middle ground: common words become single tokens, rare words get split into meaningful pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(vocab):\n",
    "    \"\"\"Count all adjacent pairs across the vocabulary.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(pair, vocab):\n",
    "    \"\"\"Merge all instances of a pair in the vocabulary.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab\n",
    "\n",
    "def train_bpe(text, num_merges=20):\n",
    "    \"\"\"Train a simple BPE tokenizer.\"\"\"\n",
    "    # Start: every word is split into characters + end-of-word marker\n",
    "    words = text.lower().split()\n",
    "    vocab = Counter()\n",
    "    for word in words:\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "\n",
    "    print(\"Initial vocabulary (top 5):\")\n",
    "    for word, freq in vocab.most_common(5):\n",
    "        print(f\"  '{word}' x{freq}\")\n",
    "    print()\n",
    "\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pairs(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        merges.append(best_pair)\n",
    "        print(f\"Merge {i+1:2d}: {best_pair[0]} + {best_pair[1]} ‚Üí {''.join(best_pair)}  (appeared {pairs[best_pair]} times)\")\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "# Train on a simple corpus\n",
    "corpus = \"\"\"\n",
    "the cat sat on the mat the cat is fat the cat sat the mat is flat\n",
    "a cat a bat a hat a rat the rat sat on the mat\n",
    "\"\"\"\n",
    "\n",
    "vocab, merges = train_bpe(corpus, num_merges=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final vocabulary\n",
    "print(\"\\nFinal vocabulary after merges:\")\n",
    "for word, freq in sorted(vocab.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  '{word}' x{freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GPT-4's Real Tokenizer\n",
    "\n",
    "GPT models use `tiktoken` with a vocabulary of ~100,000 tokens trained on a massive corpus. Let's explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-4's tokenizer (cl100k_base)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(f\"Vocabulary size: {enc.n_vocab:,} tokens\")\n",
    "\n",
    "def show_tokens(text):\n",
    "    \"\"\"Display text with token boundaries visible.\"\"\"\n",
    "    token_ids = enc.encode(text)\n",
    "    tokens = [enc.decode([t]) for t in token_ids]\n",
    "    \n",
    "    print(f\"\\nText: {repr(text)}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Tokens:    {tokens}\")\n",
    "    print(f\"Count: {len(token_ids)} tokens\")\n",
    "    return token_ids, tokens\n",
    "\n",
    "# Simple example\n",
    "show_tokens(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Surprising Edge Cases\n",
    "\n",
    "These examples reveal important truths about how LLMs actually process text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Numbers are tricky ---\n",
    "print(\"=\" * 50)\n",
    "print(\"NUMBERS\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"1\")\n",
    "show_tokens(\"100\")\n",
    "show_tokens(\"1000\")\n",
    "show_tokens(\"10000\")\n",
    "show_tokens(\"100000\")\n",
    "print(\"\\n‚Üí This is why LLMs struggle with arithmetic!\"\n",
    "      \"\\n  '10000' is not one token - the model has to reason across multiple pieces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spaces matter ---\n",
    "print(\"=\" * 50)\n",
    "print(\"SPACES CHANGE TOKENS\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"cat\")\n",
    "show_tokens(\" cat\")\n",
    "show_tokens(\"  cat\")\n",
    "print(\"\\n‚Üí The leading space is part of the token!\")\n",
    "print(\"  'cat' and ' cat' are different tokens with different IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Capitalization ---\n",
    "print(\"=\" * 50)\n",
    "print(\"CAPITALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"python\")\n",
    "show_tokens(\"Python\")\n",
    "show_tokens(\"PYTHON\")\n",
    "print(\"\\n‚Üí Same word, different tokens. The model learns each separately.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rare words get split ---\n",
    "print(\"=\" * 50)\n",
    "print(\"RARE vs COMMON WORDS\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"dog\")\n",
    "show_tokens(\"serendipity\")\n",
    "show_tokens(\"antidisestablishmentarianism\")\n",
    "show_tokens(\"supercalifragilisticexpialidocious\")\n",
    "print(\"\\n‚Üí Common short words = 1 token\")\n",
    "print(\"  Rare/long words = many tokens (the model is less 'practiced' at these)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Code tokenization ---\n",
    "print(\"=\" * 50)\n",
    "print(\"CODE\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"def hello_world():\")\n",
    "show_tokens(\"for i in range(10):\")\n",
    "show_tokens(\"import numpy as np\")\n",
    "print(\"\\n‚Üí Common Python keywords and patterns are often single tokens.\")\n",
    "print(\"  This is why GPT-4 is good at code - it's been trained on a lot of it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Other languages ---\n",
    "print(\"=\" * 50)\n",
    "print(\"NON-ENGLISH TEXT\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"Hello, how are you?\")       # English\n",
    "show_tokens(\"Hola, ¬øc√≥mo est√°s?\")         # Spanish\n",
    "show_tokens(\"Bonjour, comment allez-vous?\")  # French\n",
    "show_tokens(\"ÏïàÎÖïÌïòÏÑ∏Ïöî\")                   # Korean\n",
    "show_tokens(\"ŸÖÿ±ÿ≠ÿ®ÿß\")                      # Arabic\n",
    "print(\"\\n‚Üí English is the most token-efficient.\")\n",
    "print(\"  Other languages use more tokens for the same meaning.\")\n",
    "print(\"  This means LLMs are intrinsically better at English than other languages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Emojis ---\n",
    "print(\"=\" * 50)\n",
    "print(\"EMOJIS\")\n",
    "print(\"=\" * 50)\n",
    "show_tokens(\"I love üçï\")\n",
    "show_tokens(\"üòÄüòÇü§£\")\n",
    "show_tokens(\"üè≥Ô∏è‚Äçüåà\")\n",
    "print(\"\\n‚Üí Emojis can be multiple tokens, especially complex compound ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Token Efficiency - How Dense Is Your Text?\n",
    "\n",
    "LLMs have a **context window** limit (measured in tokens, not words). Understanding token efficiency matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_efficiency(text):\n",
    "    \"\"\"Show tokens per word ratio.\"\"\"\n",
    "    tokens = enc.encode(text)\n",
    "    words = text.split()\n",
    "    ratio = len(tokens) / max(len(words), 1)\n",
    "    print(f\"{len(tokens):4d} tokens | {len(words):4d} words | ratio {ratio:.2f} | {repr(text[:50])}\")\n",
    "\n",
    "print(\"Token efficiency comparison (tokens per word):\")\n",
    "print(\"-\" * 70)\n",
    "token_efficiency(\"The quick brown fox jumps over the lazy dog.\")\n",
    "token_efficiency(\"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\")\n",
    "token_efficiency(\"2 + 2 = 4, 100 + 200 = 300, 1234 + 5678 = 6912\")\n",
    "token_efficiency(\"http://www.example.com/path/to/resource?param1=value1&param2=value2\")\n",
    "token_efficiency(\"ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ÄÎäî ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏäµÎãàÎã§\")\n",
    "token_efficiency(\"üòÄ üéâ üçï üöÄ üí° üî• ‚≠ê üåç\")\n",
    "\n",
    "print(\"\\n‚Üí Prose English ‚âà 1.3 tokens/word\")\n",
    "print(\"  Code, URLs, numbers, non-English = much less efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: The Reversal Test\n",
    "\n",
    "A famous LLM quirk: models struggle to reverse strings. Now you know why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversal_test(word):\n",
    "    \"\"\"Show why reversing words is hard for LLMs.\"\"\"\n",
    "    tokens = enc.encode(word)\n",
    "    token_strings = [enc.decode([t]) for t in tokens]\n",
    "    \n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"Tokens: {token_strings}\")\n",
    "    \n",
    "    reversed_word = word[::-1]\n",
    "    reversed_tokens = enc.encode(reversed_word)\n",
    "    reversed_token_strings = [enc.decode([t]) for t in reversed_tokens]\n",
    "    \n",
    "    print(f\"Reversed: '{reversed_word}'\")\n",
    "    print(f\"Reversed tokens: {reversed_token_strings}\")\n",
    "    print()\n",
    "\n",
    "print(\"Why is reversing a string hard for an LLM?\")\n",
    "print(\"=\" * 50)\n",
    "reversal_test(\"hello\")\n",
    "reversal_test(\"tokenizer\")\n",
    "reversal_test(\"python\")\n",
    "\n",
    "print(\"‚Üí To reverse 'tokenizer', the model can't just reverse the token order.\")\n",
    "print(\"  'tokenizer' might be 1 token, but 'rezikenot' is a completely different set.\")\n",
    "print(\"  The model has to reason at the character level, which it wasn't designed for.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Your Turn - Explore!\n",
    "\n",
    "Try your own examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try anything you want\n",
    "my_text = \"Type something here and see how it gets tokenized!\"\n",
    "show_tokens(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tokens is your name?\n",
    "names = [\"John\", \"Mohammed\", \"Xiaoling\", \"Anastasia\", \"Bob\"]\n",
    "print(\"Token count per name:\")\n",
    "for name in names:\n",
    "    tokens = enc.encode(name)\n",
    "    decoded = [enc.decode([t]) for t in tokens]\n",
    "    print(f\"  {name:15} ‚Üí {decoded}  ({len(tokens)} token{'s' if len(tokens) > 1 else ''})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What Tokenization Tells Us About LLMs\n",
    "\n",
    "| Observation | What It Means |\n",
    "|---|---|\n",
    "| Numbers split into multiple tokens | LLMs are bad at arithmetic by design |\n",
    "| Spaces are part of tokens | Formatting and whitespace affects model behavior |\n",
    "| Non-English uses more tokens | English-centric training = English advantage |\n",
    "| Common words = 1 token | Frequent patterns are well-learned |\n",
    "| Rare words = many tokens | LLMs struggle more with unusual vocabulary |\n",
    "| Characters aren't the unit | String manipulation tasks (reversal, counting letters) are hard |\n",
    "\n",
    "**Key insight:** The tokenizer is a hidden layer between you and the model. Understanding it explains many seemingly strange LLM behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
