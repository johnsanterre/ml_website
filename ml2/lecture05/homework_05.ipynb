{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 5: Autoencoders & Embeddings - Homework\n",
        "\n",
        "**ML2: Advanced Machine Learning**\n",
        "\n",
        "**Estimated Time**: 1 hour\n",
        "\n",
        "---\n",
        "\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Programming Exercises (60%)\n",
        "\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Experiment: Compression Forces Learning\n",
        "\n",
        "**Time**: 12 min\n",
        "\n",
        "Observe how different bottleneck sizes affect reconstruction quality and feature learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded, encoded\n",
        "\n",
        "# Train autoencoders with different bottleneck sizes\n",
        "latent_dims = [2, 8, 32, 128]\n",
        "\n",
        "# TODO: Train each and observe reconstruction quality\n",
        "# Question: What happens to reconstruction as latent_dim changes?\n",
        "# What is the model learning to fit into the bottleneck?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Knowledge Questions (40%)\n",
        "\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Short Answer)\n",
        "\n",
        "**Question 1 - Why Compression Forces Learning**\n\nAn autoencoder with 784-dim input → 32-dim latent → 784-dim output must compress 784 numbers into 32.\n\nExplain:\n1. Why can't the model just memorize each input?\n2. What must it learn instead?\n3. What happens if latent_dim = 784 (no compression)?\n",
        "\n",
        "**Hint**: Compression = information bottleneck. The model must learn the ESSENCE of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\n",
        "\n",
        "**Question 2 - Latent Space as Learned Representation**\n\nAfter training an autoencoder on MNIST digits, the 32-dimensional latent space captures what makes each digit unique.\n\nExplain:\n1. Why might similar digits (like 3 and 8) be close in latent space?\n2. How is this different from pixel space (raw 784 dimensions)?\n3. What makes the latent representation 'better' than raw pixels?\n",
        "\n",
        "**Hint**: Latent space captures semantic similarity, not just pixel similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3 (Multiple Choice)\n",
        "\n",
        "**Question 3 - Reconstruction Loss**\n\nYou train an autoencoder and get: Train reconstruction loss = 0.01, Test reconstruction loss = 0.10\n\nWhat does this suggest?\n\nA) The model is working perfectly\nB) The model is overfitting\nC) The latent dimension is too large\nD) The model needs more training\n",
        "\n",
        "A) The model is working perfectly\n",
        "B) The model is overfitting\n",
        "C) The latent dimension is too large\n",
        "D) The model needs more training\n",
        "\n",
        "**Hint**: Large gap between train and test = overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4 (Short Answer)\n",
        "\n",
        "**Question 4 - Autoencoders vs Supervised Learning**\n\nAutoencoders are UNSUPERVISED - they don't need labels.\n\nExplain:\n1. What is the 'label' that an autoencoder trains on?\n2. Why is this useful when you don't have labeled data?\n3. How could you use an autoencoder's learned representations for a downstream supervised task?\n",
        "\n",
        "**Hint**: The input IS the label (reconstruct yourself). The latent space can be used for other tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5 (Short Answer)\n",
        "\n",
        "**Question 5 - VAE vs Standard Autoencoder**\n\nVariational Autoencoders (VAEs) learn a DISTRIBUTION in latent space, not just a point.\n\nExplain:\n1. Why is learning a distribution useful for GENERATION?\n2. What can VAEs do that standard autoencoders cannot?\n3. What's the tradeoff?\n",
        "\n",
        "**Hint**: Distribution = you can sample new points. Standard AE only encodes existing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6 (Multiple Choice)\n",
        "\n",
        "**Question 6 - Bottleneck Size Selection**\n\nYou're building an autoencoder for 1000x1000 images. Which latent dimension is most reasonable?\n\nA) latent_dim = 2 (extreme compression)\nB) latent_dim = 256 (moderate compression)\nC) latent_dim = 1000000 (no compression)\nD) latent_dim = 100000 (minimal compression)\n",
        "\n",
        "A) latent_dim = 2 (extreme compression)\n",
        "B) latent_dim = 256 (moderate compression)\n",
        "C) latent_dim = 1000000 (no compression)\n",
        "D) latent_dim = 100000 (minimal compression)\n",
        "\n",
        "**Hint**: Too small = loss of information. Too large = no compression benefit. Need balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7 (Short Answer)\n",
        "\n",
        "**Question 7 - Denoising Autoencoders**\n\nA denoising autoencoder is trained with: corrupted_input → encoder → decoder → clean_output\n\nExplain:\n1. Why does this make the learned features MORE robust?\n2. What additional capability does the model gain?\n3. How is this related to data augmentation?\n",
        "\n",
        "**Hint**: Learning to denoise forces the model to learn the underlying structure, not memorize noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8 (Short Answer)\n",
        "\n",
        "**Question 8 - Embeddings as Dimensionality Reduction**\n\nAutoencoder latent space, PCA, and t-SNE all reduce dimensionality. \n\nCompare:\n1. How does an autoencoder differ from PCA?\n2. When would you prefer an autoencoder over PCA?\n3. What's the computational tradeoff?\n",
        "\n",
        "**Hint**: PCA = linear. Autoencoder = nonlinear (with activation functions). PCA is faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9 (Short Answer)\n",
        "\n",
        "**Question 9 - Interpolation in Latent Space**\n\nYou encode two images to latent vectors z1 and z2. Then you decode MIDPOINT (z1 + z2)/2.\n\nWhat do you expect to see? Why is this useful?\n",
        "\n",
        "**Hint**: If latent space is smooth, the midpoint should be a blend of the two images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 10 (Short Answer)\n",
        "\n",
        "**Question 10 - Real-World Application**\n\nGoogle Photos uses learned embeddings to search photos by similarity without tags.\n\nExplain:\n1. How does an autoencoder-style approach enable this?\n2. Why is pixel-space similarity not good enough?\n3. What must the latent space capture to make semantic search work?\n",
        "\n",
        "**Hint**: Latent space must capture 'what the image contains' not 'what pixels look like'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission\n",
        "\n",
        "Before submitting:\n",
        "1. Run all cells to ensure code executes without errors\n",
        "2. Check that all questions are answered\n",
        "3. Review your explanations for clarity\n",
        "\n",
        "**To Submit**:\n",
        "- File → Download → Download .ipynb\n",
        "- Submit the notebook file to your course LMS\n",
        "\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}