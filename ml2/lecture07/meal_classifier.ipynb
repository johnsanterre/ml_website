{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meal Classification: From Random Forests to Transformers\n",
    "\n",
    "In this notebook, we'll build a classifier that predicts which cuisine a meal comes from based on its ingredients.\n",
    "\n",
    "**Learning Journey:**\n",
    "1. Generate synthetic meal data\n",
    "2. Build a Random Forest classifier (traditional ML)\n",
    "3. Build a Transformer classifier (modern deep learning)\n",
    "4. Compare the approaches\n",
    "\n",
    "**The Question:** Given a list of ingredients, can we predict the cuisine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Data - Fridge Ingredients by Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_fridge_ingredients = ['Chicken','Lentils (Dal)','Yogurt','Cumin','Tomatoes','Garlic','Onions','Ginger','Cilantro (Coriander leaves)','Green chilies','Turmeric','Mustard seeds','Garam masala','Curry leaves','Paneer (Indian cottage cheese)','Ghee (Clarified butter)','Fresh mint','Fenugreek leaves (Kasuri methi)','Spinach','Eggplant (Brinjal)','Okra (Bhindi)','Potatoes','Cauliflower','Green peas','Bell peppers','Carrots','Fresh coconut','Tamarind paste','Rice','Whole wheat flour (Atta)','Chickpeas (Chana)','Black beans (Rajma)','Butter','Milk','Eggs','Mangoes','Lemons','Lime','Jaggery','Cardamom','Cloves','Cinnamon sticks','Bay leaves','Fennel seeds','Red chili powder','Coriander powder','Black mustard oil','Asafoetida (Hing)','Pickles (Achar)','Pomegranate']\n",
    "american_fridge_ingredients = ['Milk','Eggs','Butter','Cheddar cheese','Mozzarella cheese','Yogurt','Chicken breast','Ground beef','Bacon','Ham','Sausage','Turkey','Lettuce','Spinach','Carrots','Broccoli','Bell peppers','Tomatoes','Cucumbers','Zucchini','Mushrooms','Onions','Garlic','Potatoes','Sweet potatoes','Corn','Peas','Green beans','Apples','Bananas','Grapes','Oranges','Strawberries','Blueberries','Bread','Tortillas','Pasta','Rice','Ketchup','Mustard','Mayonnaise','Ranch dressing','Barbecue sauce','Soy sauce','Hot sauce','Butter','Cream cheese','Orange juice','Apple juice','Jam']\n",
    "french_fridge_ingredients = ['Butter','Cream','Milk','Eggs','Cheese (Camembert)','Cheese (Brie)','Cheese (Roquefort)','Cheese (Gruyère)','Cheese (Comté)','Cheese (Goat cheese)','Yogurt','Chicken','Duck','Ham','Sausage','Pâté','Smoked salmon','Fresh herbs (Thyme)','Fresh herbs (Rosemary)','Fresh herbs (Tarragon)','Fresh herbs (Parsley)','Garlic','Onions','Shallots','Leeks','Carrots','Celery','Potatoes','Tomatoes','Zucchini','Eggplant','Bell peppers','Green beans','Lettuce','Spinach','Mushrooms','Baguette','Croissants','Wine (Red)','Wine (White)','Champagne','Olive oil','Balsamic vinegar','Dijon mustard','Crème fraîche','Anchovies','Capers','Cornichons','Puff pastry','Pears','Apples']\n",
    "korean_fridge_ingredients = ['Kimchi','Soy sauce','Gochujang (Korean red chili paste)','Doenjang (Fermented soybean paste)','Gochugaru (Korean red chili flakes)','Sesame oil','Sesame seeds','Garlic','Ginger','Green onions (Scallions)','Onions','Korean radish (Mu)','Napa cabbage','Spinach','Carrots','Zucchini','Cucumber','Bean sprouts','Bell peppers','Potatoes','Sweet potatoes','Tofu','Fish sauce','Oyster sauce','Rice vinegar','Mirin','Rice cakes (Tteok)','Rice','Rice noodles','Glass noodles (Dangmyeon)','Seaweed (Gim/Nori)','Dried anchovies','Beef','Pork','Chicken','Eggs','Milk','Cheese','Butter','Mushrooms (Enoki)','Mushrooms (Shiitake)','Korean pears','Apples','Asian pears','Persimmons','Chili peppers','Perilla leaves','Ssamjang (Korean dipping sauce)','Kimchi base (Mak kimchi)']\n",
    "mexican_fridge_ingredients = ['Chicken','Beef','Pork','Chorizo','Fish','Shrimp','Eggs','Milk','Cheese (Queso fresco)','Cheese (Queso Oaxaca)','Cheese (Cotija)','Butter','Crema (Mexican sour cream)','Limes','Cilantro','Jalapeños','Serrano peppers','Poblano peppers','Habanero peppers','Tomatoes','Tomatillos','Avocados','Onions','Garlic','Bell peppers','Cucumbers','Carrots','Radishes','Zucchini','Corn','Black beans','Pinto beans','Lettuce','Cabbage','Spinach','Chayote','Nopales (Cactus)','Tortillas','Tortilla chips','Salsa','Guacamole','Hot sauce','Adobo sauce','Chipotle peppers in adobo','Pickled jalapeños','Mole sauce','Mexican chocolate','Tequila','Beer']\n",
    "\n",
    "fridges = {\n",
    "    'Indian': indian_fridge_ingredients,\n",
    "    'American': american_fridge_ingredients,\n",
    "    'French': french_fridge_ingredients,\n",
    "    'Korean': korean_fridge_ingredients,\n",
    "    'Mexican': mexican_fridge_ingredients\n",
    "}\n",
    "\n",
    "print(\"Cuisines available:\", list(fridges.keys()))\n",
    "print(\"\\nIngredient counts:\")\n",
    "for cuisine, ingredients in fridges.items():\n",
    "    print(f\"  {cuisine}: {len(ingredients)} ingredients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generate Synthetic Meal Data\n",
    "\n",
    "We'll create realistic meals by:\n",
    "- Picking ingredients primarily from one cuisine (90%)\n",
    "- Sometimes adding ingredients from other cuisines (10%) for realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meal(cuisine, num_ingredients=6):\n",
    "    \"\"\"Generate a meal primarily from one cuisine.\"\"\"\n",
    "    primary_ingredients = fridges[cuisine]\n",
    "    \n",
    "    # 90% chance to pick from primary cuisine, 10% from others\n",
    "    meal_ingredients = []\n",
    "    for _ in range(num_ingredients):\n",
    "        if random.random() < 0.9:\n",
    "            # Pick from primary cuisine\n",
    "            meal_ingredients.append(random.choice(primary_ingredients))\n",
    "        else:\n",
    "            # Pick from a random other cuisine (adds noise/realism)\n",
    "            other_cuisine = random.choice([c for c in fridges.keys() if c != cuisine])\n",
    "            meal_ingredients.append(random.choice(fridges[other_cuisine]))\n",
    "    \n",
    "    return meal_ingredients\n",
    "\n",
    "# Test it\n",
    "print(\"Sample Indian meal:\", generate_meal('Indian', 5))\n",
    "print(\"Sample Korean meal:\", generate_meal('Korean', 5))\n",
    "print(\"Sample Mexican meal:\", generate_meal('Mexican', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meal_dataset(meals_per_cuisine=500):\n",
    "    \"\"\"Create a dataset of meals with labels.\"\"\"\n",
    "    meals = []\n",
    "    labels = []\n",
    "    \n",
    "    for cuisine in fridges.keys():\n",
    "        for _ in range(meals_per_cuisine):\n",
    "            # Vary meal size between 4-8 ingredients\n",
    "            num_ingredients = random.randint(4, 8)\n",
    "            meal = generate_meal(cuisine, num_ingredients)\n",
    "            meals.append(meal)\n",
    "            labels.append(cuisine)\n",
    "    \n",
    "    return meals, labels\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating meals...\")\n",
    "meals, labels = create_meal_dataset(meals_per_cuisine=500)\n",
    "print(f\"✓ Created {len(meals)} meals\")\n",
    "print(f\"✓ Cuisines: {Counter(labels)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nFirst 3 meals:\")\n",
    "for i in range(3):\n",
    "    print(f\"{labels[i]:10} → {meals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Engineering for Random Forest\n",
    "\n",
    "Random Forests need numerical features. We'll create:\n",
    "- **Bag of Words:** Binary vector indicating which ingredients are present\n",
    "- Each ingredient becomes a feature (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary of all possible ingredients\n",
    "all_ingredients = set()\n",
    "for ingredients in fridges.values():\n",
    "    all_ingredients.update(ingredients)\n",
    "\n",
    "vocab = sorted(list(all_ingredients))\n",
    "ingredient_to_idx = {ing: idx for idx, ing in enumerate(vocab)}\n",
    "\n",
    "print(f\"Total unique ingredients: {len(vocab)}\")\n",
    "print(f\"Sample ingredients: {vocab[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meal_to_features(meal):\n",
    "    \"\"\"Convert a meal (list of ingredients) to a binary feature vector.\"\"\"\n",
    "    features = np.zeros(len(vocab))\n",
    "    for ingredient in meal:\n",
    "        if ingredient in ingredient_to_idx:\n",
    "            features[ingredient_to_idx[ingredient]] = 1\n",
    "    return features\n",
    "\n",
    "# Convert all meals to feature vectors\n",
    "X = np.array([meal_to_features(meal) for meal in meals])\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"\\nExample feature vector (first meal):\")\n",
    "print(f\"Non-zero features: {np.sum(X[0])} out of {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train Random Forest Classifier\n",
    "\n",
    "Random Forests work by:\n",
    "1. Building many decision trees\n",
    "2. Each tree votes on the prediction\n",
    "3. Majority vote wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_depth=20,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"✓ Training complete\")\n",
    "\n",
    "# Evaluate\n",
    "train_pred = rf_model.predict(X_train)\n",
    "test_pred = rf_model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
    "print(f\"  Test Accuracy:  {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, test_pred, labels=list(fridges.keys()))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=fridges.keys(),\n",
    "            yticklabels=fridges.keys())\n",
    "plt.title('Random Forest - Confusion Matrix')\n",
    "plt.ylabel('True Cuisine')\n",
    "plt.xlabel('Predicted Cuisine')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - which ingredients are most predictive?\n",
    "importance_data = list(zip(vocab, rf_model.feature_importances_))\n",
    "importance_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Ingredients for Classification:\")\n",
    "print(f\"{'Ingredient':<40} {'Importance':>10}\")\n",
    "print(\"-\" * 52)\n",
    "for ingredient, importance in importance_data[:20]:\n",
    "    print(f\"{ingredient:<40} {importance:>10.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Build a Transformer Classifier\n",
    "\n",
    "Now let's use a Transformer! \n",
    "\n",
    "**Why Transformers?**\n",
    "- They can learn relationships between ingredients (self-attention)\n",
    "- Don't need manual feature engineering\n",
    "- Work directly with sequences\n",
    "\n",
    "**Architecture:**\n",
    "1. Embedding layer (ingredient → vector)\n",
    "2. Transformer encoder layer\n",
    "3. Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ingredient and label vocabularies\n",
    "ingredient_to_id = {ing: idx+1 for idx, ing in enumerate(vocab)}  # Start from 1\n",
    "ingredient_to_id['<PAD>'] = 0  # Padding token\n",
    "\n",
    "cuisine_to_id = {cuisine: idx for idx, cuisine in enumerate(fridges.keys())}\n",
    "id_to_cuisine = {idx: cuisine for cuisine, idx in cuisine_to_id.items()}\n",
    "\n",
    "print(f\"Ingredient vocab size: {len(ingredient_to_id)}\")\n",
    "print(f\"Number of cuisines: {len(cuisine_to_id)}\")\n",
    "print(f\"Cuisine mapping: {cuisine_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MealDataset(Dataset):\n",
    "    \"\"\"Dataset for meals.\"\"\"\n",
    "    def __init__(self, meals, labels):\n",
    "        self.meals = meals\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.meals)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        meal = self.meals[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert ingredients to IDs\n",
    "        ingredient_ids = [ingredient_to_id.get(ing, 0) for ing in meal]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(ingredient_ids, dtype=torch.long),\n",
    "            torch.tensor(cuisine_to_id[label], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences to same length in batch.\"\"\"\n",
    "    meals, labels = zip(*batch)\n",
    "    meals_padded = pad_sequence(meals, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return meals_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split meals into train/test (using same split as before)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "meals_train, meals_test, labels_train, labels_test = train_test_split(\n",
    "    meals, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MealDataset(meals_train, labels_train)\n",
    "test_dataset = MealDataset(meals_test, labels_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MealTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for meal classification.\"\"\"\n",
    "    def __init__(self, vocab_size, num_classes, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer: ingredient ID -> dense vector\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # Positional encoding (simple learned version)\n",
    "        self.pos_encoding = nn.Embedding(20, d_model)  # Max 20 ingredients\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embed ingredients and add positional encoding\n",
    "        x = self.embedding(x) + self.pos_encoding(positions)\n",
    "        \n",
    "        # Create padding mask (True for padding tokens)\n",
    "        padding_mask = (x.sum(dim=-1) == 0)\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Take mean of non-padded tokens\n",
    "        mask = (~padding_mask).unsqueeze(-1).float()\n",
    "        x = (x * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Create model\n",
    "model = MealTransformer(\n",
    "    vocab_size=len(ingredient_to_id),\n",
    "    num_classes=len(cuisine_to_id),\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for meals, labels in loader:\n",
    "        meals, labels = meals.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(meals)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for meals, labels in loader:\n",
    "            meals, labels = meals.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(meals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Transformer...\\n\")\n",
    "num_epochs = 20\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.3f} Acc={train_acc:.3f} | \"\n",
    "              f\"Test Loss={test_loss:.3f} Acc={test_acc:.3f}\")\n",
    "\n",
    "print(f\"\\n✓ Training complete! Best test accuracy: {best_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "_, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "# Convert IDs back to cuisine names\n",
    "pred_cuisines = [id_to_cuisine[p] for p in test_preds]\n",
    "true_cuisines = [id_to_cuisine[l] for l in test_labels]\n",
    "\n",
    "print(\"\\nTransformer Classification Report:\")\n",
    "print(classification_report(true_cuisines, pred_cuisines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Transformer\n",
    "cm_transformer = confusion_matrix(true_cuisines, pred_cuisines, labels=list(fridges.keys()))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_transformer, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=fridges.keys(),\n",
    "            yticklabels=fridges.keys())\n",
    "plt.title('Transformer - Confusion Matrix')\n",
    "plt.ylabel('True Cuisine')\n",
    "plt.xlabel('Predicted Cuisine')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare Random Forest vs Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Random Forest Test Accuracy:  {test_acc:.3f}\")\n",
    "print(f\"Transformer Test Accuracy:    {best_acc:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"\\n1. Random Forest:\")\n",
    "print(\"   - Treats each ingredient independently\")\n",
    "print(\"   - Fast to train\")\n",
    "print(\"   - Provides feature importance\")\n",
    "print(\"   - Good for tabular data\")\n",
    "\n",
    "print(\"\\n2. Transformer:\")\n",
    "print(\"   - Learns relationships between ingredients (attention)\")\n",
    "print(\"   - Can understand ingredient combinations\")\n",
    "print(\"   - More flexible for sequence data\")\n",
    "print(\"   - Requires more data to shine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Test on New Meals\n",
    "\n",
    "Let's create some test meals and see what both models predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_both_models(meal_ingredients):\n",
    "    \"\"\"Predict cuisine using both models.\"\"\"\n",
    "    print(f\"\\nMeal: {meal_ingredients}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Random Forest prediction\n",
    "    rf_features = meal_to_features(meal_ingredients)\n",
    "    rf_pred = rf_model.predict([rf_features])[0]\n",
    "    rf_proba = rf_model.predict_proba([rf_features])[0]\n",
    "    \n",
    "    print(\"Random Forest Prediction:\")\n",
    "    print(f\"  Predicted: {rf_pred}\")\n",
    "    for i, cuisine in enumerate(fridges.keys()):\n",
    "        print(f\"    {cuisine:10} {rf_proba[i]:.3f}\")\n",
    "    \n",
    "    # Transformer prediction\n",
    "    meal_ids = [ingredient_to_id.get(ing, 0) for ing in meal_ingredients]\n",
    "    meal_tensor = torch.tensor([meal_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(meal_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)[0]\n",
    "        pred_idx = outputs.argmax(1).item()\n",
    "    \n",
    "    transformer_pred = id_to_cuisine[pred_idx]\n",
    "    \n",
    "    print(\"\\nTransformer Prediction:\")\n",
    "    print(f\"  Predicted: {transformer_pred}\")\n",
    "    for i, cuisine in enumerate(fridges.keys()):\n",
    "        print(f\"    {cuisine:10} {probs[i].item():.3f}\")\n",
    "\n",
    "# Test some meals\n",
    "test_meals = [\n",
    "    ['Kimchi', 'Rice', 'Gochujang (Korean red chili paste)', 'Garlic', 'Sesame oil'],\n",
    "    ['Tortillas', 'Avocados', 'Limes', 'Cilantro', 'Tomatoes'],\n",
    "    ['Butter', 'Cheese (Brie)', 'Wine (Red)', 'Garlic', 'Fresh herbs (Thyme)'],\n",
    "    ['Cumin', 'Turmeric', 'Garam masala', 'Yogurt', 'Chicken'],\n",
    "    ['Bacon', 'Eggs', 'Cheddar cheese', 'Bread', 'Butter']\n",
    "]\n",
    "\n",
    "for meal in test_meals:\n",
    "    predict_with_both_models(meal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We built two classifiers for cuisine prediction:\n",
    "\n",
    "**Random Forest:**\n",
    "- Traditional ML approach\n",
    "- Uses hand-crafted features (bag-of-words)\n",
    "- Fast and interpretable\n",
    "- Works well for this task\n",
    "\n",
    "**Transformer:**\n",
    "- Modern deep learning approach\n",
    "- Learns features automatically\n",
    "- Uses attention to find ingredient relationships\n",
    "- More flexible and scalable\n",
    "\n",
    "Both approaches work well for this problem! The choice depends on:\n",
    "- Data size (Transformers need more data)\n",
    "- Interpretability needs (Random Forest is clearer)\n",
    "- Complexity of relationships (Transformers handle complex patterns better)\n",
    "- Computational resources (Random Forest is faster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
