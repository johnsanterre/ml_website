{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 7: Sequence Models & Attention - Homework\\n",
        "\\n",
        "**ML2: Advanced Machine Learning**\\n",
        "\\n",
        "**Estimated Time**: 1 hour\\n",
        "\\n",
        "---\\n",
        "\\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\\n",
        "\\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\\n",
        "import matplotlib.pyplot as plt\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "\\n",
        "# Set random seed for reproducibility\\n",
        "np.random.seed(42)\\n",
        "torch.manual_seed(42)\\n",
        "\\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Part 1: Programming Exercises (60%)\\n",
        "\\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Implement Simple RNN\\n",
        "\\n",
        "**Time**: 20 min\\n",
        "\\n",
        "Build a basic RNN cell from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch",
        "import torch.nn as nn",
        "",
        "class SimpleRNN(nn.Module):",
        "    def __init__(self, input_size, hidden_size, output_size):",
        "        super(SimpleRNN, self).__init__()",
        "        # TODO: Define parameters",
        "        pass",
        "    ",
        "    def forward(self, x, hidden=None):",
        "        # TODO: Implement RNN cell",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Attention Mechanism\\n",
        "\\n",
        "**Time**: 20 min\\n",
        "\\n",
        "Implement scaled dot-product attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch",
        "import torch.nn.functional as F",
        "",
        "def scaled_dot_product_attention(query, key, value):",
        "    # TODO: Implement attention",
        "    # scores = QK^T / sqrt(d_k)",
        "    # weights = softmax(scores)",
        "    # output = weights * V",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Part 2: Knowledge Questions (40%)\\n",
        "\\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Multiple Choice)\\n",
        "\\n",
        "Why do LSTMs help with the vanishing gradient problem compared to vanilla RNNs?\\n",
        "\\n",
        "A) They have more parameters\\n",
        "B) They use gates to control information flow\\n",
        "C) They train faster\\n",
        "D) They don't use backpropagation\\n",
        "\\n",
        "**Hint**: Think about the cell state and gate mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\\n",
        "\\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\\n",
        "\\n",
        "Explain the key innovation of the attention mechanism. Why is it better than using only the last hidden state?\\n",
        "\\n",
        "**Hint**: Consider long sequences and information bottleneck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\\n",
        "\\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\\n",
        "## Submission\\n",
        "\\n",
        "Before submitting:\\n",
        "1. Run all cells to ensure code executes without errors\\n",
        "2. Check that all questions are answered\\n",
        "3. Review your explanations for clarity\\n",
        "\\n",
        "**To Submit**:\\n",
        "- File → Download → Download .ipynb\\n",
        "- Submit the notebook file to your course LMS\\n",
        "\\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}