{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 7: Introduction to Transformers - Homework\n",
        "\n",
        "**ML2: Advanced Machine Learning**\n",
        "\n",
        "**Estimated Time**: 1 hour\n",
        "\n",
        "---\n",
        "\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Programming Exercises (60%)\n",
        "\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Experiment: Attention Weights Visualization\n",
        "\n",
        "**Time**: 10 min\n",
        "\n",
        "See how attention dynamically weighs different words based on context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Simplified attention example\n",
        "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "# Simple word embeddings (3-dim for visualization)\n",
        "embeddings = torch.randn(6, 3)  # 6 words, 3 dimensions\n",
        "\n",
        "def scaled_dot_product_attention(query, keys, values):\n",
        "    # query: (d_k,), keys: (seq_len, d_k), values: (seq_len, d_v)\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(keys, query) / np.sqrt(d_k)  # (seq_len,)\n",
        "    attention_weights = F.softmax(scores, dim=0)  # (seq_len,)\n",
        "    output = torch.matmul(attention_weights, values)  # (d_v,)\n",
        "    return output, attention_weights\n",
        "\n",
        "# For word \"cat\", what does it attend to?\n",
        "query_word_idx = 1  # \"cat\"\n",
        "query = embeddings[query_word_idx]\n",
        "keys = embeddings\n",
        "values = embeddings\n",
        "\n",
        "output, weights = scaled_dot_product_attention(query, keys, values)\n",
        "\n",
        "print(f\"Attention weights when processing '{sentence[query_word_idx]}':\")\n",
        "for i, (word, weight) in enumerate(zip(sentence, weights)):\n",
        "    print(f\"  {word}: {weight:.3f}\")\n",
        "\n",
        "# TODO: What pattern do you see? Why does \"cat\" attend to certain words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Knowledge Questions (40%)\n",
        "\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Short Answer)\n",
        "\n",
        "**Question 1 - Static vs Context-Dependent Embeddings**\n\nWord2Vec: \"bank\" always has the same embedding\nTransformer: \"bank\" has DIFFERENT embeddings in \"river bank\" vs \"financial bank\"\n\nExplain:\n1. How does the transformer create context-dependent embeddings?\n2. What role does attention play in this?\n3. Why is this a major breakthrough?\n",
        "\n",
        "**Hint**: Attention looks at surrounding words to dynamically adjust the representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\n",
        "\n",
        "**Question 2 - Attention as Dynamic Weighting**\n\nIn \"The cat sat on the mat\", when processing \"sat\", attention might heavily weight \"cat\" (subject) and \"mat\" (object).\n\nExplain:\n1. Why is this better than fixed-size context windows (like n-grams)?\n2. What can attention capture that RNNs struggle with?\n3. How does this help with long-range dependencies?\n",
        "\n",
        "**Hint**: Attention can connect words that are far apart without sequential processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3 (Multiple Choice)\n",
        "\n",
        "**Question 3 - Query, Key, Value Intuition**\n\nAttention uses three vectors: Query, Key, Value\n\nThink of it like a search engine:\n- Query = what you're looking for\n- Keys = indexed items\n- Values = the actual content you retrieve\n\nWhich is correct?\n\nA) Query comes from the word being processed, Keys come from all words\nB) All three are the same vector\nC) Keys and Values are always different\nD) Query is learned, Keys are fixed\n",
        "\n",
        "A) Query comes from the word being processed, Keys come from all words\n",
        "B) All three are the same vector\n",
        "C) Keys and Values are always different\n",
        "D) Query is learned, Keys are fixed\n",
        "\n",
        "**Hint**: Query = what this word is asking for. Keys = what other words offer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4 (Short Answer)\n",
        "\n",
        "**Question 4 - Why Scale by sqrt(d_k)?**\n\nScaled dot-product attention: scores = (Q·K^T) / sqrt(d_k)\n\nExplain: Why divide by sqrt(d_k)? What problem does this solve?\n",
        "\n",
        "**Hint**: Dot products grow large in high dimensions, pushing softmax into saturation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5 (Short Answer)\n",
        "\n",
        "**Question 5 - Multi-Head Attention**\n\nTransformers use MULTIPLE attention heads in parallel.\n\nExplain:\n1. Why use multiple heads instead of one big attention mechanism?\n2. What might different heads learn to specialize in?\n3. How does this relate to ensemble learning?\n",
        "\n",
        "**Hint**: Different heads can capture different relationships (syntax, semantics, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6 (Multiple Choice)\n",
        "\n",
        "**Question 6 - Positional Encoding**\n\nTransformers process all words in parallel (unlike RNNs which are sequential).\n\nWhat problem does this create, and how do positional encodings solve it?\n\nA) Memory usage - solved by compression\nB) Loss of word order information - solved by adding position signals\nC) Slow training - solved by parallelization\nD) Overfitting - solved by regularization\n",
        "\n",
        "A) Memory usage - solved by compression\n",
        "B) Loss of word order information - solved by adding position signals\n",
        "C) Slow training - solved by parallelization\n",
        "D) Overfitting - solved by regularization\n",
        "\n",
        "**Hint**: Without sequential processing, how does the model know word order?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7 (Short Answer)\n",
        "\n",
        "**Question 7 - Self-Attention vs Cross-Attention**\n\nSelf-attention: A sentence attends to itself\nCross-attention: One sequence attends to another (e.g., translation)\n\nExplain: In an English-to-French translator, where would you use each type?\n",
        "\n",
        "**Hint**: Self-attention within English and French. Cross-attention from French to English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8 (Short Answer)\n",
        "\n",
        "**Question 8 - Computational Complexity**\n\nFor a sequence of length n, self-attention has O(n²) complexity.\n\nExplain:\n1. Why O(n²)? (What computation causes this?)\n2. Why is this a problem for long documents?\n3. How might you reduce this for very long sequences?\n",
        "\n",
        "**Hint**: Every word attends to every other word = n×n comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9 (Short Answer)\n",
        "\n",
        "**Question 9 - Attention vs RNN Sequential Processing**\n\nRNNs: Process word 1, then 2, then 3... (sequential)\nTransformers: Process all words in parallel\n\nExplain:\n1. What's the training speed advantage of transformers?\n2. What's the tradeoff in memory usage?\n3. Why can't RNNs parallelize as easily?\n",
        "\n",
        "**Hint**: RNNs need previous hidden state. Transformers compute all positions independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 10 (Short Answer)\n",
        "\n",
        "**Question 10 - Real-World Application**\n\nGPT, BERT, and T5 are all transformers with billions of parameters.\n\nExplain:\n1. Why did transformers enable scaling to billions of parameters when RNNs couldn't?\n2. What property of attention makes it more effective at scale?\n3. What's the cost of this scalability?\n",
        "\n",
        "**Hint**: Parallelization + long-range dependencies. Cost = computation and memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission\n",
        "\n",
        "Before submitting:\n",
        "1. Run all cells to ensure code executes without errors\n",
        "2. Check that all questions are answered\n",
        "3. Review your explanations for clarity\n",
        "\n",
        "**To Submit**:\n",
        "- File → Download → Download .ipynb\n",
        "- Submit the notebook file to your course LMS\n",
        "\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}