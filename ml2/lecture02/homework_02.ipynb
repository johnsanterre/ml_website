{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2: Neural Networks & Backpropagation - Homework\n",
        "\n",
        "**ML2: Advanced Machine Learning**\n",
        "\n",
        "**Estimated Time**: 1 hour\n",
        "\n",
        "---\n",
        "\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Programming Exercises (60%)\n",
        "\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Experiment: Tracing the Chain Rule\n",
        "\n",
        "**Time**: 12 min\n",
        "\n",
        "This experiment demonstrates how backpropagation systematically applies the chain rule. You'll manually trace gradients through a simple computation graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Simple computation graph: z = (x * w + b)^2\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = x * w + b  # Intermediate value\n",
        "z = y ** 2      # Final output\n",
        "\n",
        "print(f\"Forward pass: x={x.item()}, w={w.item()}, b={b.item()}\")\n",
        "print(f\"Intermediate y = x*w + b = {y.item()}\")\n",
        "print(f\"Final z = y^2 = {z.item()}\")\n",
        "\n",
        "# Backpropagation (automatic)\n",
        "z.backward()\n",
        "\n",
        "print(f\"\\nAutomatic gradients:\")\n",
        "print(f\"dz/dx = {x.grad.item()}\")\n",
        "print(f\"dz/dw = {w.grad.item()}\")\n",
        "print(f\"dz/db = {b.grad.item()}\")\n",
        "\n",
        "# TODO: Now manually compute these gradients using the chain rule:\n",
        "# dz/dx = dz/dy * dy/dx\n",
        "# What is dz/dy? What is dy/dx?\n",
        "# Verify your manual calculation matches PyTorch's automatic result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Experiment: Gradient Descent with Different Batch Sizes\n",
        "\n",
        "**Time**: 10 min\n",
        "\n",
        "Compare how different batch sizes affect training dynamics. Observe convergence speed, stability, and final loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data: y = 2x + 1 + noise\n",
        "torch.manual_seed(42)\n",
        "X = torch.randn(1000, 1) * 10\n",
        "y = 2 * X + 1 + torch.randn(1000, 1) * 2\n",
        "\n",
        "def train_with_batch_size(batch_size, epochs=50):\n",
        "    model = nn.Linear(1, 1)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle and create batches\n",
        "        perm = torch.randperm(len(X))\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for i in range(0, len(X), batch_size):\n",
        "            batch_idx = perm[i:i+batch_size]\n",
        "            X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        losses.append(epoch_loss / num_batches)\n",
        "    \n",
        "    return losses, model.weight.item(), model.bias.item()\n",
        "\n",
        "# Compare different batch sizes\n",
        "batch_sizes = [1, 32, 256, 1000]  # SGD, mini-batch, large batch, full batch\n",
        "results = {}\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    losses, final_w, final_b = train_with_batch_size(bs)\n",
        "    results[bs] = {'losses': losses, 'w': final_w, 'b': final_b}\n",
        "    print(f\"Batch size {bs}: Final w={final_w:.3f}, b={final_b:.3f}\")\n",
        "\n",
        "# TODO: After running, answer reflection questions about what you observe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Knowledge Questions (40%)\n",
        "\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Short Answer)\n",
        "\n",
        "**Question 1 - The Chain Rule IS Backpropagation (Conceptual)**\n\nConsider a 3-layer network: Input → Layer1 → Layer2 → Layer3 → Loss\n\nTo update Layer1's weights, you need dLoss/dWeights_Layer1.\n\nExplain:\n1. Why must you compute gradients for Layer3, then Layer2, THEN Layer1 (in that order)?\n2. What mathematical principle requires this backward ordering?\n3. What would go wrong if you tried to compute Layer1's gradient first?\n",
        "\n",
        "**Hint**: Think about the chain rule: df/dx = df/dg × dg/dx. What do you need to know first?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\n",
        "\n",
        "**Question 2 - Manual Chain Rule Calculation**\n\nBased on the 'Tracing the Chain Rule' experiment:\n\nFor the computation graph z = (x*w + b)², manually calculate dz/dw step by step:\n\n1. What is dz/dy (where y = x*w + b)?\n2. What is dy/dw?\n3. Using the chain rule, what is dz/dw = dz/dy × dy/dw?\n4. Verify this matches the PyTorch automatic gradient.\n",
        "\n",
        "**Hint**: Remember: d/dy(y²) = 2y and d/dw(x*w + b) involves x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3 (Short Answer)\n",
        "\n",
        "**Question 3 - Why Backward Propagation?**\n\nExplain why we call it \"backpropagation\" rather than just \"gradient computation.\"\n\nWhat specific property of the chain rule makes the backward direction necessary and efficient?\n",
        "\n",
        "**Hint**: Consider: could you compute gradients going forward instead? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4 (Multiple Choice)\n",
        "\n",
        "**Question 4 - Vanishing Gradients**\n\nIn a very deep network (100 layers), gradients in early layers become extremely small. This is called the \"vanishing gradient problem.\"\n\nWhy does this happen?\n\nA) Early layers have fewer parameters\nB) The chain rule multiplies many numbers less than 1 together\nC) Early layers receive less data\nD) Backpropagation doesn't reach early layers\n",
        "\n",
        "A) Early layers have fewer parameters\n",
        "B) The chain rule multiplies many numbers less than 1 together\n",
        "C) Early layers receive less data\n",
        "D) Backpropagation doesn't reach early layers\n",
        "\n",
        "**Hint**: Think about what happens when you multiply 0.5 × 0.5 × 0.5 × 0.5... many times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5 (Short Answer)\n",
        "\n",
        "**Question 5 - Gradient as Directional Information**\n\nA gradient magnitude of 0.001 vs 100.0 tells you very different things about the loss landscape.\n\nExplain:\n1. What does a gradient magnitude of 0.001 indicate?\n2. What does a gradient magnitude of 100.0 indicate?\n3. Which situation might require adjusting the learning rate, and how?\n",
        "\n",
        "**Hint**: Large gradient = steep slope. Small gradient = flat region (or near minimum)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6 (Short Answer)\n",
        "\n",
        "**Question 6 - Batch Size Experiment Reflection**\n\nAfter running the 'Gradient Descent with Different Batch Sizes' experiment:\n\nCompare batch_size=1 vs batch_size=1000:\n1. Which had smoother loss curves?\n2. Which made more weight updates per epoch?\n3. Which do you think explored the solution space better? Why?\n",
        "\n",
        "**Hint**: More updates = more opportunities to correct course, but also more noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7 (Short Answer)\n",
        "\n",
        "**Question 7 - The Bias-Variance Tradeoff in Batch Size**\n\nLarge batch sizes give accurate gradient estimates (low variance) but fewer updates.\nSmall batch sizes give noisy estimates (high variance) but more frequent updates.\n\nExplain: Why might the NOISE from small batches actually be BENEFICIAL for finding better solutions?\n",
        "\n",
        "**Hint**: Think about escaping local minima. Can noise help you 'jump out' of a bad spot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8 (Multiple Choice)\n",
        "\n",
        "**Question 8 - Learning Rate Impact**\n\nIf your learning rate is too large, what typically happens?\n\nA) Training is slow but converges smoothly\nB) The loss oscillates wildly or increases\nC) Gradients become more accurate\nD) The model memorizes the training data\n",
        "\n",
        "A) Training is slow but converges smoothly\n",
        "B) The loss oscillates wildly or increases\n",
        "C) Gradients become more accurate\n",
        "D) The model memorizes the training data\n",
        "\n",
        "**Hint**: Think about taking huge steps in the direction of the gradient. Can you overshoot the minimum?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9 (Short Answer)\n",
        "\n",
        "**Question 9 - Connecting Backpropagation to Learning**\n\nIntegrate the concepts:\n\nExplain how backpropagation (chain rule) and gradient descent work together to enable learning.\n\nYour answer should connect:\n- How backpropagation computes what gradients are\n- How gradient descent uses those gradients to update weights\n- Why you need both for deep learning to work\n",
        "\n",
        "**Hint**: Backpropagation tells you the direction, gradient descent tells you how far to step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 10 (Short Answer)\n",
        "\n",
        "**Question 10 - Real-World Implications**\n\nModern deep learning models (like GPT or BERT) have billions of parameters across hundreds of layers.\n\nExplain: \n1. Why is automatic differentiation (backpropagation) absolutely essential for training these models?\n2. What would be impossible without it?\n",
        "\n",
        "**Hint**: Imagine manually calculating gradients for 175 billion parameters. How long would that take?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission\n",
        "\n",
        "Before submitting:\n",
        "1. Run all cells to ensure code executes without errors\n",
        "2. Check that all questions are answered\n",
        "3. Review your explanations for clarity\n",
        "\n",
        "**To Submit**:\n",
        "- File → Download → Download .ipynb\n",
        "- Submit the notebook file to your course LMS\n",
        "\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}