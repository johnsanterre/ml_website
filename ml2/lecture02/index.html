<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 2: Neural Networks Fundamentals & Backpropagation | SMU</title>
    <style>
        /* Modern Reset & Base */
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            background-color: #f7f9fc;
            color: #333;
            line-height: 1.6;
        }

        /* Header / Navbar */
        .site-header {
            background-color: #354CA1; /* SMU Blue */
            color: white;
            padding: 1rem 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 2rem;
        }

        .header-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .site-title {
            font-size: 1.25rem;
            font-weight: 500;
            color: white;
            text-decoration: none;
            opacity: 0.9;
        }
        
        .site-title:hover {
            opacity: 1;
        }

        .nav-link {
            background-color: rgba(255,255,255,0.15);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            text-decoration: none;
            font-weight: 500;
            font-size: 0.9rem;
            transition: background-color 0.2s;
        }

        .nav-link:hover {
            background-color: rgba(255,255,255,0.25);
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px 60px;
        }

        /* Content Box */
        .content-box {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            margin-bottom: 30px;
            border-top: 5px solid #CC0035; /* SMU Red accent */
        }

        h1 {
            color: #354CA1;
            font-size: 2.2rem;
            margin-bottom: 0.5em;
            padding-bottom: 15px;
            border-bottom: 1px solid #eee;
        }

        h2 {
            color: #333;
            font-size: 1.5rem;
            margin-top: 1.0em;
            margin-bottom: 0.4em;
        }

        h3 {
            color: #555;
            font-size: 1.2rem;
            margin-top: 0.8em;
            margin-bottom: 0.5em;
        }

        p {
            margin-bottom: 0.6em;
            color: #444;
        }

        ul, ol {
            padding-left: 1.5em;
            margin-bottom: 0.8em;
            color: #444;
        }

        li {
            margin-bottom: 0.25em;
        }

        a {
            color: #CC0035;
            text-decoration: none;
            font-weight: 500;
        }

        a:hover {
            text-decoration: underline;
        }

        /* Top Resources Links */
        .top-resources {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            padding: 20px 0;
            margin-bottom: 20px;
            border-bottom: 1px solid #eee;
        }

        .resource-btn {
            padding: 8px 16px;
            border-radius: 4px;
            font-size: 0.85rem;
            font-weight: 600;
            text-decoration: none;
            transition: all 0.2s;
            background-color: white;
            color: #354CA1;
            border: 1px solid #354CA1;
            text-align: center;
        }

        .resource-btn:hover {
            background-color: #f0f4ff;
            text-decoration: none;
        }

        /* Resources Section within Content */
        .resources {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 8px;
            margin-top: 3em;
            border: 1px solid #eee;
            border-left: 4px solid #354CA1;
        }

        .resources h2 {
            margin-top: 0;
            color: #354CA1;
            font-size: 1.3rem;
            margin-bottom: 15px;
        }
        
    </style>
</head>

<body>

    <!-- Professional Navbar -->
    <header class="site-header">
        <div class="header-container">
            <a href="../../index.html" class="site-title">‚Üê Course Overview</a>
            <span style="font-size: 0.9rem; opacity: 0.8;">ML2: Advanced Machine Learning</span>
        </div>
    </header>

    <div class="container">

        <div class="content-box">
<h1>Week 2: Neural Networks Fundamentals & Backpropagation</h1>

            <div class="top-resources">
                <a href="#" class="resource-btn">Videos</a>
                <a href="../../output/ml2/lecture02/ml2_week02.pdf" class="resource-btn">Textbook</a>
                <a href="https://colab.research.google.com/github/johnsanterre/ml_website/blob/main/ml2/lecture02/week_02_exercises.ipynb" class="resource-btn" target="_blank">Colab</a>
                <a href="https://colab.research.google.com/" class="resource-btn" target="_blank">Colab 2</a>
            </div>

            <p><strong>Course Generated Slides:</strong> <a href="../../topics/backpropagation/backpropagation_slides.html" target="_blank">Backpropagation</a> | <a href="../../topics/gradient_descent/gradient_descent_slides.html" target="_blank">Gradient Descent</a> | <a href="../../topics/learning_rate_schedulers/learning_rate_schedulers_slides.html" target="_blank">Learning Rate Schedulers</a> | <a href="../../topics/search_algorithms/search_algorithms_slides.html" target="_blank">Search Algorithms</a></p>

            <section>
                <h2>1. Neural Network Training</h2>
                <ul>
                    <li>Detailed look at forward propagation
                        <ul>
                            <li>Building on single neuron concept</li>
                            <li>Matrix operations in neural networks</li>
                            <li>Computational graphs</li>
                        </ul>
                    </li>
                    <li>Introduction to backpropagation
                        <ul>
                            <li>Chain rule application</li>
                            <li>Computing gradients</li>
                            <li>Error propagation through layers</li>
                        </ul>
                    </li>
                    <li>Gradient descent fundamentals
                        <ul>
                            <li>Step sizes</li>
                            <li>Learning rate concept</li>
                            <li>Batch vs mini-batch vs stochastic</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section>
                <h2>2. Loss Functions in Depth</h2>
                <ul>
                    <li>Mean Squared Error (MSE)
                        <ul>
                            <li>Mathematical formulation</li>
                            <li>Use cases in regression</li>
                        </ul>
                    </li>
                    <li>Cross-Entropy Loss
                        <ul>
                            <li>Binary cross-entropy</li>
                            <li>Categorical cross-entropy</li>
                            <li>Use cases in classification</li>
                        </ul>
                    </li>
                    <li>Implementing loss functions in PyTorch</li>
                    <li>Loss function selection criteria</li>
                </ul>
            </section>

            <section>
                <h2>3. Gradient-Based Optimization</h2>
                <ul>
                    <li>Computing gradients
                        <ul>
                            <li>Automatic differentiation</li>
                            <li>Manual gradient calculation examples</li>
                        </ul>
                    </li>
                    <li>Gradient descent variations
                        <ul>
                            <li>Stochastic Gradient Descent (SGD)</li>
                            <li>Mini-batch gradient descent</li>
                        </ul>
                    </li>
                    <li>Common challenges
                        <ul>
                            <li>Vanishing gradients</li>
                            <li>Exploding gradients</li>
                            <li>Local minima and saddle points</li>
                        </ul>
                    </li>
                </ul>
            </section>

            
            <div class="resources">
                <h2>Required Reading</h2>
                <ul>
                    <li>Deep Learning Book (Goodfellow et al.) - Chapter 6: Deep Feedforward Networks</li>
                    <li>Deep Learning Book (Goodfellow et al.) - Chapter 8: Optimization for Training Deep Models</li>
                </ul>

                <h2>Additional Reading</h2>
                <ul>
                    <li><a href="https://sinews.siam.org/Details-Page/deep-learning-in-scientific-computing-understanding-the-instability-mystery" target="_blank">Deep Learning in Scientific Computing: Understanding the Instability Mystery</a> - Analysis of neural network training stability and optimization challenges</li>
                    <li><a href="https://en.m.wikipedia.org/wiki/General_linear_group" target="_blank">General Linear Group</a> - Mathematical foundations of linear transformations and their properties</li>
                    <li><a href="https://distill.pub/2020/bayesian-optimization/" target="_blank">Exploring Bayesian Optimization</a> - A visual and intuitive guide to understanding Bayesian optimization for hyperparameter tuning</li>
                </ul>

                <h2>Practical Resources</h2>
                <ul>
                    <li><a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py" target="_blank">Neural Networks Implementation</a> - Michael Nielsen's educational implementation of neural networks with backpropagation</li>
                </ul>

                <h2>Learning Objectives</h2>
                <ul>
                    <li>Master the mathematics behind forward and backward propagation</li>
                    <li>Understand different loss functions and their applications</li>
                    <li>Implement basic gradient descent optimization</li>
                    <li>Identify and address common training challenges</li>
                </ul>

                
            </div>
            </div>
        </div>
    </div>
</body>
</html>