{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6: From Autoencoders to Embeddings - Homework\n",
        "\n",
        "**ML2: Advanced Machine Learning**\n",
        "\n",
        "**Estimated Time**: 1 hour\n",
        "\n",
        "---\n",
        "\n",
        "This homework combines programming exercises and knowledge-based questions to reinforce this week's concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('✓ Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Programming Exercises (60%)\n",
        "\n",
        "Complete the following programming tasks. Read each description carefully and implement the requested functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Experiment: Word Embeddings Capture Semantics\n",
        "\n",
        "**Time**: 10 min\n",
        "\n",
        "Explore how word embeddings encode semantic relationships through vector arithmetic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simplified word embeddings (in reality, these are 300-dim, but using 3-dim for clarity)\n",
        "embeddings = {\n",
        "    'king': np.array([0.9, 0.1, 0.8]),\n",
        "    'queen': np.array([0.9, 0.9, 0.7]),\n",
        "    'man': np.array([0.8, 0.1, 0.3]),\n",
        "    'woman': np.array([0.8, 0.9, 0.2]),\n",
        "    'prince': np.array([0.85, 0.15, 0.75]),\n",
        "    'princess': np.array([0.85, 0.85, 0.65])\n",
        "}\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def find_closest(target_vec, embeddings, exclude=[]):\n",
        "    best_word, best_sim = None, -1\n",
        "    for word, vec in embeddings.items():\n",
        "        if word in exclude:\n",
        "            continue\n",
        "        sim = cosine_similarity(target_vec, vec)\n",
        "        if sim > best_sim:\n",
        "            best_word, best_sim = word, sim\n",
        "    return best_word, best_sim\n",
        "\n",
        "# Analogy: king - man + woman ≈ ?\n",
        "result_vec = embeddings['king'] - embeddings['man'] + embeddings['woman']\n",
        "word, similarity = find_closest(result_vec, embeddings, exclude=['king', 'man', 'woman'])\n",
        "\n",
        "print(f\"king - man + woman ≈ {word} (similarity: {similarity:.3f})\")\n",
        "\n",
        "# TODO: Try other analogies. What pattern emerges?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Knowledge Questions (40%)\n",
        "\n",
        "Answer the following questions to test your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 (Short Answer)\n",
        "\n",
        "**Question 1 - Distributional Hypothesis**\n\n\"You shall know a word by the company it keeps\" - Firth (1957)\n\nWord embeddings are learned from word co-occurrence patterns.\n\nExplain:\n1. Why do words with similar MEANING end up with similar VECTORS?\n2. How does this differ from a one-hot encoding?\n3. What does this tell you about what language models learn?\n",
        "\n",
        "**Hint**: Words used in similar contexts (appear near similar words) get similar embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2 (Short Answer)\n",
        "\n",
        "**Question 2 - Vector Arithmetic Magic**\n\nking - man + woman ≈ queen\n\nThis works because embeddings capture semantic relationships as directions in vector space.\n\nExplain:\n1. What direction in vector space does (king - man) represent?\n2. Why does adding 'woman' give you 'queen'?\n3. What does this reveal about how embeddings encode relationships?\n",
        "\n",
        "**Hint**: (king - man) ≈ 'royalty' direction. Adding 'woman' = royal + female."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3 (Multiple Choice)\n",
        "\n",
        "**Question 3 - Static vs Contextual Embeddings**\n\nWord2Vec gives the word \"bank\" the SAME embedding whether it means \"river bank\" or \"financial bank\".\n\nWhat problem does this create?\n\nA) Too much memory usage\nB) Loss of word-sense disambiguation\nC) Slower computation\nD) Unable to handle rare words\n",
        "\n",
        "A) Too much memory usage\n",
        "B) Loss of word-sense disambiguation\n",
        "C) Slower computation\n",
        "D) Unable to handle rare words\n",
        "\n",
        "**Hint**: Static = one vector per word, ignoring context. This is a problem for polysemous words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4 (Short Answer)\n",
        "\n",
        "**Question 4 - Skip-gram vs CBOW**\n\nWord2Vec has two architectures:\n- Skip-gram: Predict context from target word\n- CBOW: Predict target word from context\n\nExplain:\n1. Which would work better for rare words?\n2. Which is computationally more efficient?\n3. Why might they learn slightly different embeddings?\n",
        "\n",
        "**Hint**: Skip-gram gets more training examples per occurrence (multiple context words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5 (Short Answer)\n",
        "\n",
        "**Question 5 - Negative Sampling Efficiency**\n\nNaive Word2Vec would compute softmax over 50,000+ vocabulary words for each training example. This is slow.\n\nNegative sampling: Instead, sample a few negative examples and use binary classification.\n\nExplain: How does this make training tractable?\n",
        "\n",
        "**Hint**: Binary classification on 5-10 words vs softmax over 50,000 words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6 (Multiple Choice)\n",
        "\n",
        "**Question 6 - Embedding Dimension Selection**\n\nWord2Vec embeddings are typically 300-dimensional. What happens if you use 5 dimensions? What about 10,000?\n\nA) 5-dim loses semantic information, 10,000-dim overfits\nB) 5-dim is better (simpler), 10,000-dim is worse\nC) Dimension doesn't matter\nD) Larger is always better\n",
        "\n",
        "A) 5-dim loses semantic information, 10,000-dim overfits\n",
        "B) 5-dim is better (simpler), 10,000-dim is worse\n",
        "C) Dimension doesn't matter\n",
        "D) Larger is always better\n",
        "\n",
        "**Hint**: Too small = can't capture complexity. Too large = overfitting and computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**: [Write your answer here - e.g., 'B']\n",
        "\n",
        "**Explanation**: [Explain why this is correct]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7 (Short Answer)\n",
        "\n",
        "**Question 7 - GloVe vs Word2Vec**\n\nWord2Vec: Learn from local context windows\nGloVe: Learn from global co-occurrence statistics\n\nExplain: What's the conceptual difference? Why might GloVe capture certain relationships better?\n",
        "\n",
        "**Hint**: Global statistics = counts of how often words appear together across entire corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8 (Short Answer)\n",
        "\n",
        "**Question 8 - Bias in Embeddings**\n\nWord embeddings trained on web text show gender bias:\n\"doctor - man + woman ≈ nurse\"\n\nExplain:\n1. Why do embeddings encode societal biases?\n2. Is this a problem? When?\n3. How might you mitigate this?\n",
        "\n",
        "**Hint**: Embeddings learn from data. If data contains bias, embeddings will too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9 (Short Answer)\n",
        "\n",
        "**Question 9 - Subword Embeddings (FastText)**\n\nFastText learns embeddings for CHARACTER N-GRAMS, not just whole words.\n\nExample: \"running\" = [\"run\", \"runn\", \"unni\", \"nnin\", \"ning\"]\n\nExplain: How does this help with rare words or typos?\n",
        "\n",
        "**Hint**: Even if you've never seen 'joggen', you can compose it from n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 10 (Short Answer)\n",
        "\n",
        "**Question 10 - Real Application**\n\nGoogle Translate uses word embeddings as a first step before translation.\n\nExplain:\n1. Why are embeddings better than one-hot encodings for translation?\n2. How do embeddings help with zero-shot translation (translating between languages you didn't train on)?\n",
        "\n",
        "**Hint**: Embeddings capture semantic similarity. Similar concepts in different languages cluster together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer**:\n",
        "\n",
        "[Write your answer here in 2-4 sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission\n",
        "\n",
        "Before submitting:\n",
        "1. Run all cells to ensure code executes without errors\n",
        "2. Check that all questions are answered\n",
        "3. Review your explanations for clarity\n",
        "\n",
        "**To Submit**:\n",
        "- File → Download → Download .ipynb\n",
        "- Submit the notebook file to your course LMS\n",
        "\n",
        "**Note**: Make sure your name is in the filename (e.g., homework_01_yourname.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}