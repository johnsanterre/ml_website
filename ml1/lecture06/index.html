<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Trees and Random Forests - ML1</title>
    <style>
        body {
            font-family: "Times New Roman", Times, serif;
            background-color: #000000;
            color: #ffffff;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        .content-box {
            background: #0f0f0f;
            padding: 30px;
            border-radius: 8px;
            margin: 20px 0;
        }

        h1, h2, h3 {
            color: #ffffff;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }

        h1 {
            font-size: 2em;
            border-bottom: 2px solid #ff9900;
            padding-bottom: 0.3em;
        }

        h2 {
            font-size: 1.5em;
            color: #ff9900;
        }

        h3 {
            font-size: 1.2em;
            color: #ffb84d;
        }

        ul, ol {
            padding-left: 1.5em;
            margin: 1em 0;
        }

        li {
            margin: 0.5em 0;
        }

        ul ul {
            margin: 0.2em 0;
        }

        a {
            color: #ff9900;
            text-decoration: none;
            transition: color 0.3s ease;
        }

        a:hover {
            color: #ffb84d;
        }

        .back-button {
            display: inline-block;
            padding: 8px 16px;
            background: none;
            border: 1px solid #ff9900;
            color: #ff9900;
            border-radius: 4px;
            text-decoration: none;
            margin-bottom: 20px;
            transition: all 0.3s ease;
        }

        .back-button:hover {
            background: #ff9900;
            color: #000000;
        }

        .resources {
            background: #1f1f1f;
            padding: 20px;
            border-radius: 8px;
            margin-top: 2em;
        }

        .resources h2 {
            margin-top: 0;
        }

        .textbook-button {
            display: inline-block;
            padding: 10px 20px;
            background: #ff9900;
            color: #000000;
            text-decoration: none;
            border-radius: 4px;
            margin-top: 20px;
            font-weight: bold;
            margin-right: 10px;
        }

        .textbook-button:hover {
            background: #ffb84d;
        }

        .top-resources {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="../../index.html" class="back-button">‚Üê Back to Course List</a>
        
        <div class="top-resources">
            <a href="#" class="textbook-button" target="_blank">Videos</a>
            <a href="output/ml1/lecture06/ml1_week06.pdf" class="textbook-button">Textbook</a>
            <a href="https://colab.research.google.com/" class="textbook-button" target="_blank">Colab</a>
        </div>

        <div class="content-box">
            <h1>Week 6: Decision Trees and Random Forests</h1>

            <section>
                <h2>Overview</h2>
                <p>This week covers decision trees from fundamentals through advanced concepts, including tree construction, optimization, and ensemble methods. The progression to random forests demonstrates how combining multiple models can leverage the "wisdom of crowds" principle to improve performance.</p>
            </section>

            <section>
                <h2>Learning Objectives</h2>
                <ul>
                    <li>Understand decision tree fundamentals</li>
                    <li>Implement tree construction algorithms</li>
                    <li>Master splitting criteria and their derivations</li>
                    <li>Apply pruning techniques effectively</li>
                    <li>Understand ensemble learning principles</li>
                    <li>Apply collective decision-making concepts</li>
                </ul>
            </section>

            <section>
                <h2>1. Decision Tree Foundations</h2>
                <ul>
                    <li>Core Concepts
                        <ul>
                            <li>Tree structure and terminology</li>
                            <li>Node types and functions</li>
                            <li>Decision boundaries</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section>
                <h2>2. Tree Construction Process</h2>
                <ul>
                    <li>Building Algorithm
                        <ul>
                            <li>Recursive splitting</li>
                            <li>Feature selection methods</li>
                            <li>Threshold optimization</li>
                        </ul>
                    </li>
                    <li>Split Criteria
                        <ul>
                            <li>Gini impurity derivation</li>
                            <li>Information gain calculation</li>
                            <li>Entropy-based measures</li>
                            <li>Comparison of methods</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section>
                <h2>3. Tree Optimization</h2>
                <ul>
                    <li>Stopping Criteria
                        <ul>
                            <li>Maximum depth rules</li>
                            <li>Minimum sample thresholds</li>
                            <li>Purity requirements</li>
                        </ul>
                    </li>
                    <li>Pruning Strategies
                        <ul>
                            <li>Chi-squared pruning</li>
                        </ul>
                    </li>
                    <li>Decision Boundaries
                        <ul>
                            <li>Boundary visualization</li>
                            <li>Geometric interpretation</li>
                            <li>Boundary optimization</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section>
                <h2>4. Tree Generalization</h2>
                <ul>
                    <li>Model Improvement
                        <ul>
                            <li>Overfitting prevention</li>
                            <li>Cross-validation strategies</li>
                            <li>Error estimation</li>
                        </ul>
                    </li>
                    <li>Ensemble Methods Progression
                        <ul>
                            <li>Ensemble principles</li>
                            <li>Wisdom of crowds effect</li>
                            <li>Bagging concepts</li>
                            <li>Feature randomization</li>
                            <li>Diversity in decision making</li>
                            <li>Voting mechanisms</li>
                        </ul>
                    </li>
                    <li>Advanced Boosting
                        <ul>
                            <li>Random Forest to XGBoost</li>
                            <li>Sequential learning</li>
                            <li>Gradient boosting concepts</li>
                            <li>Optimization improvements</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <div class="resources">
                <h2>Key Takeaways</h2>
                <ul>
                    <li>Decision trees provide interpretable solutions</li>
                    <li>Split criteria choice impacts tree performance</li>
                    <li>Pruning is essential for generalization</li>
                    <li>Collective wisdom improves model performance</li>
                    <li>Diverse ensemble members enhance predictions</li>
                </ul>

                <h2>Practical Exercises</h2>
                <ul>
                    <li>Implement decision tree from scratch</li>
                    <li>Compare different split criteria</li>
                    <li>Apply various pruning techniques</li>
                    <li>Explore ensemble diversity and performance</li>
                    <li>Compare individual vs collective predictions</li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html> 